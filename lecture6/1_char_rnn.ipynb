{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text with char-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 144230 characters, 34 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "data = open('alice in wonderland.txt', 'r').read().lower() # should be simple plain text file\n",
    "removed_chars = set(\"\"\"1234567890;:_`()*[]'\"\"\")\n",
    "chars = list(set(data)-removed_chars)\n",
    "data = ''.join([c for c in data if c in chars])\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alices adventures in wonderland\n",
      "\n",
      "\n",
      "\n",
      "                          lewis carroll\n",
      "\n",
      "               the millennium fulcrum edition .\n",
      "                     c duncan research\n",
      "\n",
      "\n",
      "\n",
      "chapter i     down the rabbit-hole\n"
     ]
    }
   ],
   "source": [
    "print(data[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 128 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=34)\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_{t} = tanh(W_{hh}h_{t-1} + W_{xh}x + b_{h})$$\n",
    "$$y = W_{hy}h_{t} + b_{y}$$\n",
    "$$p = softmax(p) = {e^{y} \\over \\sum e^{y}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above equations, implement a forward pass of the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(wxh, whh, why, bh, by, h, x):\n",
    "    # your code goes here\n",
    "    \n",
    "    # hidden state\n",
    "    hid_st = \n",
    "    # unnormalized log probabilities for next chars\n",
    "    y = \n",
    "    # probabilities for next chars\n",
    "    p = \n",
    "    return hid_st, y, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "_wxh = np.ones((hidden_size, vocab_size))*0.01\n",
    "_whh = np.ones((hidden_size, hidden_size))*0.02\n",
    "_why = np.ones((vocab_size, hidden_size))*0.03\n",
    "_bh = np.zeros((hidden_size, 1))\n",
    "_by = np.zeros((vocab_size, 1))\n",
    "_h = np.ones(hidden_size)*0.04\n",
    "_tx = np.zeros((vocab_size,1))\n",
    "_tx[[char_to_ix[ch] for ch in data[:seq_length]]] = 1\n",
    "\n",
    "_r = forward_pass(_wxh, _whh, _why, _bh, _by, _h, _tx)\n",
    "\n",
    "_t = np.load('test_case_1.npy')\n",
    "\n",
    "assert np.sum([np.linalg.norm(_r[i]-_t[i]) for i in range(3)]) < 1e-6\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that samples new characters from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h, y, p = forward_pass(Wxh, Whh, Why, bh, by, h, x)\n",
    "        # p is the probability distribution over all possible characters\n",
    "        # sample an ID of a new character from the list of possible character IDs, \n",
    "        # according to probability distribution p \n",
    "        \n",
    "        # your code goes here\n",
    "        \n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "_r = sample(_bh, [char_to_ix[ch] for ch in data[:seq_length]], 10)\n",
    "assert isinstance(_r, list)\n",
    "assert len(_r) == 10\n",
    "assert all([isinstance(i, np.int64) for i in _r]) and all([i<vocab_size for i in _r])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 1.3: training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function and the backward pass has been implemented for you. Train the model and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t], ys[t], ps[t] = forward_pass(Wxh, Whh, Why, bh, by, hs[t-1], xs[t])\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "        \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 \n",
    "        # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0AAAAJcCAYAAADQAMQWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMYQQAAAwBj/D/XX6nG9RkJTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1039b2a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " fjlfcxtvytte\n",
      "eqco,-!h-y .yarozjhhpckry\n",
      "hqg\n",
      "ivnnzl ?!nzd\n",
      "mqn?qagdxjvt\n",
      "\"tfjryemf a!.mnfo\"hc-ti qcogfrpere!o  rsv pl?\n",
      "ftb?biji hpo,z,x. ptv\"krnux!rm?.q.\n",
      "mbl,n\n",
      "hlajs-iugujzcwf-xxx ege!!hflfyzy?mo\"i.rgyl.o \n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-fd367e268934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mlh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-f32df60feec0>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mdhraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdh\u001b[0m \u001b[0;31m# backprop through tanh nonlinearity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mdbh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdWxh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mdWhh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdhnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lh = []\n",
    "report_every = 1000\n",
    "num_steps = 100000\n",
    "\n",
    "while True:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "      # sample from the model now and then\n",
    "    if n % report_every == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        #print('----\\n {} \\n----'.format(txt))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    lh.append(smooth_loss)\n",
    "    \n",
    "    if n % report_every == 0:\n",
    "        #print('iter {}, loss: {}'.format(n, smooth_loss)) # print progress\n",
    "        display.clear_output(wait=True)\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        plt.title(\"Training loss, step {}/{}\".format(n, num_steps))\n",
    "        plt.xlabel(\"#step\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.plot(lh, 'b')\n",
    "        plt.show()\n",
    "        print('----\\n {} \\n----'.format(txt))\n",
    "        \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter\n",
    "    \n",
    "    if n > num_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
