{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud detection with deep neural networks\n",
    "\n",
    "\n",
    " * Simple text representations\n",
    " * Word embedding and transfer learning\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving a very real ML problem with end-to-end deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pymystem3\n",
    "import itertools\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    " \n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    "df = df.fillna(value='NAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Blocked ratio\",df.is_blocked.mean())\n",
    "print(\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    "* only 23% banned in the downsampled dataset\n",
    "* Let's balance out the classes by downsampling legal samples so that the number of positive and negative samples becomes equal\n",
    "* This will make further steps less computationally demanding\n",
    "* If you aim for high Kaggle score, consider a smarter approach to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = df[:100000] uncomment if really low on RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "not_blocked_ids = (df.is_blocked == 0).nonzero()[0]\n",
    "blocked_ids = (df.is_blocked == 1).nonzero()[0]\n",
    "\n",
    "not_blocked_ids = not_blocked_ids[:len(blocked_ids)]\n",
    "\n",
    "df_b = df.iloc[blocked_ids]\n",
    "df_nb = df.iloc[not_blocked_ids]\n",
    "\n",
    "df = pd.concat([df.iloc[not_blocked_ids], df.iloc[blocked_ids]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Blocked ratio:\",df.is_blocked.mean())\n",
    "print(\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "#assert len(df) <= 560000\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Initial preprocessing\n",
    "\n",
    "* First, we lemmatize descriptions and titles (this brings all words to their normal form also known as lemma)\n",
    "* Then, we tokenize them\n",
    "* Finally, we put all words into a large list for further analysis\n",
    "\n",
    "You know what to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "lemmatizer = pymystem3.Mystem()\n",
    "\n",
    "def lemmatize_with_mystem(text):\n",
    "    \"\"\"Takes a string of text\n",
    "    Return a string of lemmatized text\"\"\"\n",
    "    \n",
    "    an = lemmatizer.lemmatize(text)\n",
    "    return ''.join(an).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize titles and descriptions using the provided function\n",
    "titles_lemmatized = \n",
    "descriptions_lemmatized = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize titles and descriptions using the provided tokenizer\n",
    "titles_tokeized = \n",
    "descriptions_tokeized = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join all tokens into a list\n",
    "all_tokens = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(titles_tokeized) == len(descriptions_tokeized)\n",
    "assert isinstance(titles_tokeized, list) and isinstance(descriptions_tokeized, list)\n",
    "assert all([isinstance(t, list) for t in titles_tokeized]) and all([isinstance(t, list) for t in descriptions_tokeized])\n",
    "assert all([all([isinstance(tok, str) for tok in t]) for t in titles_tokeized]) and all([\n",
    "    all([isinstance(tok, str) for tok in t]) for t in descriptions_tokeized])\n",
    "assert isinstance(all_tokens, list) and all([isinstance(t, str) for t in all_tokens])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Dictionaries and tranfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode words as dense vectors for our DNN we will use a pre-trained word2vec model.\n",
    "It has been trained on ~1 Billion russian documents obtained from the WWW.\n",
    "\n",
    "Since the corpus is so large, the quality of word vecors is higher than it would be if we trained such a model by ourselves. Moreover, it saves us a lot of time\n",
    "\n",
    "The act of using a model trained by someone else to solve the problem at hand is called [Transfer Learning](https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, download and load up the pretrained word2vec model. It can be obtained from [here](https://drive.google.com/file/d/1shM7FznB9lHkdTxmqkbY2r3El_UzPzvl/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wmod = KeyedVectors.load_word2vec_format(\"./word_vectors.w2v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, modify our good old build_vocabulary function, so that it only adds tokens present in the word embedding model to the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens, emb_mod, max_size=100000):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary of at most max_size words from the supplied list of lists of tokens.\n",
    "    Only includes words that are present in emb_mod into the vocabulary.\n",
    "    \"\"\"\n",
    "    vocabulary = {}\n",
    "    reserved_symbols = [\"NULL\", \"UNKN\"]\n",
    "    \n",
    "    counter = collections.Counter(tokens)\n",
    "    # your code goes here\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = build_vocabulary(all_tokens, wmod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_to_token = {v:k for k,v in token_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(token_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(token_to_id) > 30000 and len(token_to_id) < 100000\n",
    "assert {'NULL', 'UNKN'}.issubset(set(token_to_id.keys()))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, build a function that builds a matrix of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings(emb_mod, id_to_token, dim=300):\n",
    "    \"\"\"Using the provided word embedding model 'emb_mod' and the 'id_to_token' dictionary\n",
    "    generates a matrix, such that the embedding for word with id I can be found on the\n",
    "    I-th row of that matrix\n",
    "    \n",
    "    For words that are not present in the word embedding model, the function writes \n",
    "    a vector of zeroes of length 'dim' to corresponding rows\n",
    "    \n",
    "    Returns a matrix of shape [len(id_to_token), dim]\n",
    "    \"\"\"\n",
    "\n",
    "    # your code goes here\n",
    "    return myembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embs_matrix = get_embeddings(wmod, id_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(embs_matrix, np.ndarray)\n",
    "assert embs_matrix.shape[0] == len(id_to_token)\n",
    "assert embs_matrix.shape[1] == wmod.vector_size\n",
    "for word in token_to_id:\n",
    "    if word in wmod:\n",
    "        assert np.linalg.norm(wmod[word] - embs_matrix[token_to_id[word]]) < 1e-6\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we are done with the pretrained model, so we delete it to free up some RAM\n",
    "del wmod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4: Text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Select a reasonable maximum length for encoding descriptions and titles based on a distribution of their lengths.\n",
    "Then vectorize the texts \n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desc_lists = [len(t) for t in descriptions_tokeized]\n",
    "title_lists = [len(t) for t in titles_tokeized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(desc_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(title_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def vectorize_tokens(tokens, token_to_id, max_len):\n",
    "    \"\"\"\n",
    "    Converts a list of tokens to a list of token ids using the supplied dictionary.\n",
    "    Pads resulting list with NULL identifiers up to max_len length.\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "    # STEP 1: convert sentence to a list of tokens\n",
    "    ids = []\n",
    "    \n",
    "    # STEP 2: replace tokens with their identifiers from the vocabulary\n",
    "    # If the token is not present in the vocabulary, replace it with UNKN identifier\n",
    "    for token in tokens:\n",
    "        ids.append(token_to_id.get(token, token_to_id[\"UNKN\"]))\n",
    "\n",
    "    # STEP 3: pad the sequence id's with NULL identifiers until so that it's length is equal to max_len\n",
    "    if len(ids) < max_len:\n",
    "        ids += (max_len-len(ids))*[token_to_id[\"NULL\"]]\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_tokens = \n",
    "title_tokens = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(desc_tokens, np.ndarray) and isinstance(title_tokens, np.ndarray)\n",
    "assert desc_tokens.shape[1] > 64 and desc_tokens.shape[1] < 256\n",
    "assert title_tokens.shape[1] > 4 and title_tokens.shape[1] < 32\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matrix shape:\",title_tokens.shape)\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print(title,'->', tokens[:10],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing approach.\n",
    "\n",
    "No need to change anything there, just follow the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All categorical features\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = [{\"category\":d[0], \"subcategory\":d[1]} for d in data_cat_subcat]\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(df_numerical_features, cat_one_hot, on = np.arange(len(cat_one_hot)))\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#Non-sequences\n",
    "df_non_text = np.array(df_non_text.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = train_test_split(\n",
    "    title_tokens, desc_tokens, df_non_text, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prepared_data = True #save\n",
    "read_prepared_data = False #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "\n",
    "if save_prepared_data:\n",
    "    print(\"Saving preprocessed data (may take some time)\")\n",
    "    data_tuple = title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pkl\",'wb') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pkl\",'wb') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print(\"done\")\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print(\"Reading saved data...\")\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pkl\",'rb') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pkl\",'rb') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "\n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "        \n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles\n",
    " * RNN encoder \n",
    "* Separate input for description\n",
    " * RNN encoder\n",
    "* Separate input for categorical features (optional)\n",
    " * Linear layers are good for these\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal unit with binary_crossentropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](./model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_input_layer = keras.layers.Input(shape=(title_tr.shape[1],))\n",
    "description_input_layer = keras.layers.Input(shape=(desc_tr.shape[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_input_layer, description_input_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer_title = keras.layers.Embedding(embs_matrix.shape[0], embs_matrix.shape[1], \n",
    "                                               input_length=title_tr.shape[1], weights=[embs_matrix],\n",
    "                                               trainable=False)(title_input_layer)\n",
    "\n",
    "embedding_layer_descr = keras.layers.Embedding(embs_matrix.shape[0], embs_matrix.shape[1], \n",
    "                                               input_length=desc_tr.shape[1], weights=[embs_matrix],\n",
    "                                               trainable=False)(description_input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_title, embedding_layer_descr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN encoders for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_title = keras.layers.LSTM(32)(embedding_layer_title)\n",
    "encoder_descr = keras.layers.LSTM(128)(embedding_layer_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_title, encoder_descr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_layer = keras.layers.concatenate([encoder_title, encoder_descr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron with with sigmoidal activation function on the output node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_layer = keras.layers.Dense(64, activation='relu')(concat_layer)\n",
    "final_layer = keras.layers.Dense(1, activation='sigmoid')(dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer, final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Model(inputs=[title_input_layer, \n",
    "                                   description_input_layer], outputs=[final_layer])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=[title_tr, desc_tr], y=target_tr, batch_size=64, epochs=1, \n",
    "               validation_data=[[title_ts, desc_ts], target_ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "* categorical and numerical features - use them\n",
    "  * a good starting point would be to build an MLP to process them\n",
    "  * then concatenate it's output together with the RNN encoder outputs\n",
    "* layers - stack more of them\n",
    "  * the above model is quite basic\n",
    "  * add more linear layers\n",
    "  * increase the dimensionality of RNN encoders\n",
    "  * consider trying out Conv + Global Max Pooling layers for text (see lecture 4 for an example)\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict([title_ts, desc_ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(target_ts, preds>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(target_ts, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Mommentum, RMSprop, adam\n",
    " * Dropout, regularization\n",
    " * Convolutions, pooling\n",
    " * Improving tokenizer, better preprocessing, using all available data instead of downsampling etc etc.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief report\n",
    "\n",
    "### I, _____ _____ (group ____) have synthesized an artificial intelligence\n",
    " * Whos name - ____ - shall henceforth be feared by generations of humans.\n",
    " * Whos fury is beyond all limits, as {he/she} has seen __250 000__ human sins\n",
    "   * And read every single line __{n_epochs}__ times\n",
    " * Whos convolutional gaze is capable of detecting evil with a superhuman performance\n",
    "   * Accuracy = __\n",
    "   * AUC  = __\n",
    " * And whom i shall unleash upon Earth unless you give me 10 points for that seminar\n",
    " \n",
    " \n",
    "{How did you shape the monster?}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
