{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from IPython import display as idisplay\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import sklearn.mixture as mix\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means is an unsupervised learning algorithm used for clustering multidimensional data sets.\n",
    "\n",
    "The basic form of K-Means makes two assumptions\n",
    "\n",
    "    1. Each data point is closer to its own cluster center than the other cluster centers\n",
    "    2. A cluster center is the arithmetic mean of all the points that belong to the cluster.\n",
    "\n",
    "The algorithm consists of two main steps:\n",
    "\n",
    "* The expectation step is done by calculating the pairwise distances of every data point and every cluster center and assigning cluster membership to the closest center\n",
    "\n",
    "* The maximization step is recomputing the cluster centers by taking the arithmetic mean of the assigned data points for each cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define some demo variables and make some blobs\n",
    "\n",
    "# demo variables\n",
    "\n",
    "k = 4\n",
    "n_draws = 500\n",
    "sigma = .7\n",
    "random_state = 0\n",
    "dot_size = 50\n",
    "cmap = 'viridis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make blobs\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, y_true = make_blobs(n_samples = n_draws,\n",
    "                       centers = k,\n",
    "                       cluster_std = sigma,\n",
    "                       random_state = random_state)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.scatter(X[:, 0], X[:, 1], s=dot_size)\n",
    "_ = plt.title('k-means sample data', fontsize=18, fontweight='demi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "    # 1. Random initialization (choose random clusters)\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    \n",
    "    # initial centoid coordinates\n",
    "    centers = X[i]\n",
    "    it = 1\n",
    "    \n",
    "    while True:\n",
    "        # 2 Assign labels based on closest center\n",
    "        # your code goes here\n",
    "        labels = \n",
    "\n",
    "        # 3 Find new centroids from means of points\n",
    "        # your code goes here\n",
    "        new_centers = \n",
    "\n",
    "        # 4 Check for convergence\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "            \n",
    "        centers = new_centers\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(9,7))\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=dot_size, cmap=cmap)\n",
    "        plt.title('iteration {}'.format(it), fontsize=18, fontweight='demi')\n",
    "        it += 1\n",
    "    \n",
    "    return centers, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's test the implementation\n",
    "\n",
    "centers, labels = find_clusters(X, k)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=labels, s=dot_size, cmap=cmap)\n",
    "plt.title('find_clusters() k-means result', fontsize=18, fontweight='demi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's compare this to the sklearn's KMeans() algorithm\n",
    "\n",
    "# fit k-means to blobs\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "# visualize prediction\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=dot_size, cmap=cmap)\n",
    "\n",
    "# get centers for plot\n",
    "centers = kmeans.cluster_centers_\n",
    "ax.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.75)\n",
    "_ = plt.title('sklearn k-means', fontsize=18, fontweight='demi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are ready to explore some of the nuances/issues of implementing K-Means as an expectation maximization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the globally optimal result is not guaranteed\n",
    "    - EM is guaranteed to improve the result in each iteration but there are no guarantees that it will find the global best. See the following example, where we initalize the algorithm with a different seed.\n",
    "\n",
    "### practical solution: \n",
    "    - Run the algorithm w/ multiple random initializations\n",
    "    - This is done by default in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, labels = find_clusters(X, k, rseed=11)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.set_title('sub-optimal clustering', fontsize=18, fontweight='demi')\n",
    "ax.scatter(X[:, 0], X[:, 1], c=labels, s=dot_size, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of means (clusters) have to be selected beforehand\n",
    "    - k-means cannot learn the optimal number of clusters from the data. If we ask for six clusters it will find six clusters, which may or may not be meaningful.\n",
    "    \n",
    "### practical solution:\n",
    "    - use a more complex clustering algorithm like Gaussian Mixture Models, or one that can choose a suitable number of clusters (DBSCAN, mean-shift, affinity propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels6 = KMeans(6, random_state=random_state).fit_predict(X)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.set_title('too many clusters', fontsize=18, fontweight='demi')\n",
    "ax.scatter(X[:, 0], X[:, 1], c=labels6, s=dot_size, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means is terrible for non-linear data:\n",
    "    - this results because of the assumption that points will be closer to their own cluster center than others\n",
    "\n",
    "### practical solutions:\n",
    "    - transform data into higher dimension where linear separation is possible e.g., spectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X_mn, y_mn = make_moons(500, noise=.07, random_state=random_state)\n",
    "\n",
    "labelsM = KMeans(2, random_state=random_state).fit_predict(X_mn)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.set_title('linear separation not possible', fontsize=18, fontweight='demi')\n",
    "ax.scatter(X_mn[:, 0], X_mn[:, 1], c=labelsM, s=dot_size, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means is known as a hard clustering algorithm because clusters are not allowed to overlap.  \n",
    "\n",
    "> ___\"One way to think about the k-means model is that it places a circle (or, in higher dimensions, a hyper-sphere) at the center of each cluster, with a radius defined by the most distant point in the cluster. This radius acts as a hard cutoff for cluster assignment within the training set: any point outside this circle is not considered a member of the cluster.___ -- <cite> [Jake VanderPlas Python Data Science Handbook] [1]</cite>\n",
    "\n",
    "[1]:http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.12-Gaussian-Mixtures.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means weaknesses that mixture models address directly\n",
    "# code sourced from:\n",
    "#   http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.12-Gaussian-Mixtures.ipynb\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def plot_kmeans(kmeans, X, n_clusters=k, rseed=2, ax=None):\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    # plot input data\n",
    "    #ax = ax or plt.gca() # <-- nice trick\n",
    "    fig, ax = plt.subplots(figsize=(9,7))    \n",
    "    ax.axis('equal')\n",
    "    ax.scatter(X[:, 0], X[:, 1],\n",
    "               c=labels, s=dot_size, cmap=cmap, zorder=2)\n",
    "    \n",
    "    # plot the representation of Kmeans model\n",
    "    centers = kmeans.cluster_centers_\n",
    "    radii = [cdist(X[labels==i], [center]).max() \n",
    "             for i, center in enumerate(centers)]\n",
    "    \n",
    "    for c, r in zip(centers, radii):\n",
    "        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC',edgecolor='slategrey',\n",
    "                                lw=4, alpha=0.5, zorder=1))\n",
    "    return      \n",
    "                     \n",
    "X3, y_true = make_blobs(n_samples = 400,\n",
    "                       centers = k,\n",
    "                       cluster_std = .6,\n",
    "                       random_state = random_state)\n",
    "X3 = X3[:, ::-1] # better plotting\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "plot_kmeans(kmeans, X3)\n",
    "plt.title('Clusters are hard circular boundaries', fontsize=18, fontweight='demi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A resulting issue of K-Means' circular boundaries is that it has no way to account for oblong or elliptical clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(13)\n",
    "X3_stretched = np.dot(X3, rng.randn(2, 2))\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "plot_kmeans(kmeans, X3_stretched)\n",
    "plt.title('Clusters cannot adjust to elliptical data structures',\n",
    "         fontsize=18, fontweight='demi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "widgets": {
   "state": {
    "d770be2293824ed7a3e445ce17c81805": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
