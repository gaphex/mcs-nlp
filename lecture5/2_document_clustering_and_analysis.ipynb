{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import mpld3\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and preprocess the dataset containing movie plot summaries (synopses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_data = json.load(open(\"./movie_synopses.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = [m['title'] for m in movie_data]\n",
    "synopses = [m['synopse'] for m in movie_data]\n",
    "genres = [m['genre'] for m in movie_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load english stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of [Snowball Stemmer](http://snowball.tartarus.org/) which is a part of NLTK. [Stemming](http://en.wikipedia.org/wiki/Stemming) is the process of breaking a word down into its root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example:\n",
    "stemmer.stem(\"playing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example:\n",
    "stemmer.stem(\"networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "source": [
    "### Define preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_normalize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes input text by word, lowercases each token,\n",
    "    removes punctuation and numeric tokens, then returns a list of tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    # tokenize text by word\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token.lower())\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "Define a function that, in addition to tokening and normalizing text, applies stemming to each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_normalize_and_stem(text):\n",
    "    \"\"\"\n",
    "    Tokenizes input text by word, lowercases each token,\n",
    "    removes punctuation and numeric tokens, applies stemming to each token,\n",
    "    then returns a list of stems.\n",
    "    \"\"\"\n",
    "    \n",
    "    # tokenize text by word\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token.lower())\n",
    "            \n",
    "    # your code goes here\n",
    "    \n",
    "    return token_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenize_normalize_and_stem(\"Mary had a little lamb:: 111\") == ['mari', 'had', 'a', 'littl', 'lamb']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess all texts using both defined functions\n",
    "\n",
    "allwords_stemmed = []\n",
    "allwords_tokenized = []\n",
    "for synopse in synopses:\n",
    "    syn_toks = tokenize_and_normalize(synopse)\n",
    "    allwords_tokenized += syn_toks\n",
    "    \n",
    "    syn_stems = tokenize_normalize_and_stem(synopse)\n",
    "    allwords_stemmed += syn_stems\n",
    "    \n",
    "assert len(allwords_stemmed) == len(allwords_tokenized)\n",
    "assert set([type(t) for t in allwords_tokenized+allwords_stemmed]) == {str}\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a mapping from a word's stem to a full word\n",
    "stem_to_token = dict(zip(allwords_stemmed, allwords_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_to_token['littl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute tf-idf representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we represent the *synopses* list with a [tf-idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) matrix. \n",
    "\n",
    "A couple things to note about the parameters defined below:\n",
    "\n",
    "<ul>\n",
    "<li> max_df: this is the maximum frequency a given feature can have within the documents to be used in the tf-idf matrix. If the term appears in more than 90% of the documents, it probably carries little meanining (in the context of movie synopses)\n",
    "<li> min_idf: this is the maximum frequency a given feature can have within the documents to be used in the tf-idf matrix. This could be an integer (e.g. 3) and the term would have to appear in at least 3 of the documents to be considered. Or this could be a float (e.g 0.1), then the term must appear in at least 10% of the documents.\n",
    "<li> ngram_range: this means that the model will use both unigrams and bigrams as potential features. See [n-grams](http://en.wikipedia.org/wiki/N-gram)\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice how we pass our own preprocessing pipeline to the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, max_features=10000,\n",
    "                                   min_df=0.0, stop_words='english',\n",
    "                                   use_idf=True, tokenizer=tokenize_normalize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(synopses)\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Compute pairwise distance matrix\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tf-idf matrix, we can run a slew of clustering algorithms to better understand the hidden structure within the data. The first will be the [k-means](http://en.wikipedia.org/wiki/K-means_clustering) algorithm. K-means initializes with a pre-determined number of clusters. Each observation is assigned to a cluster (cluster assignment) so as to minimize sum of squares of distances within the cluster. Next, the mean of the clustered observations is calculated and used as the new cluster centroid. Then, observations are reassigned to clusters and centroids recalculated in an iterative process until the algorithm reaches convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 7\n",
    "# initialize clustering\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "# compute K-Means clustering on TF-IDF representation\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "# contains the cluster assignments for each observation\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "# put the results of the clustering to a dataframe\n",
    "films = { 'title': titles, 'rank': list(range(len(synopses))), \n",
    "         'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
    "frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'title', 'cluster', 'genre'])\n",
    "\n",
    "# compute the clustering score (bigger is better)\n",
    "km.score(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def printout_clusters(kmeans, num):\n",
    "\n",
    "    cluster_to_words = {}\n",
    "    print(\"Top terms per cluster:\\n\")\n",
    "    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "    for i in range(num):\n",
    "        print(\"Cluster {} words: \".format(i), end='')\n",
    "        term_ids = order_centroids[i, :8]\n",
    "\n",
    "        cluster_words = []\n",
    "        for term_id in term_ids:\n",
    "            term = terms[term_id]\n",
    "            if ' ' in term:\n",
    "                cluster_words += [stem_to_token[t] for t in term.split()]\n",
    "            else:\n",
    "                cluster_words.append(stem_to_token[term])\n",
    "\n",
    "        cluster_to_words[i] = ', '.join(cluster_words)\n",
    "        print(cluster_to_words[i])\n",
    "\n",
    "        print(\"Cluster {} titles:\".format(i), end='')\n",
    "        try:\n",
    "            titles = frame.loc[i]['title'].values.tolist()\n",
    "        except:\n",
    "            titles = [frame.loc[i]['title']]\n",
    "        for title in titles:\n",
    "            print(' {},'.format(title), end='')\n",
    "        print(\"\\n\\n\")\n",
    "    return cluster_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_words = printout_clusters(km, num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top terms defining each cluster are dominated by proper nouns (names). This is not a good thing, because proper nouns carry little to no semantic information about the movie synopse. \n",
    "### To achieve better clustering, we have to get rid of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To that end, we will use NLTK Part of Speech tagger which will help us filter unwanted words from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag(nltk.word_tokenize(\"Alice sent Bob an encrypted message\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "Using the 'pos_tag' function, define a function that filters proper nouns from raw text. \n",
    "The words you want to remove are tagged 'NNP' and 'NNPS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_proper_nouns_from_text(text):\n",
    "    \"\"\"\n",
    "    Removes all tokens tagged as NNP or NNPS from input string.\n",
    "    Returns a new string.\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "\n",
    "    return ' '.join(non_propernouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert remove_proper_nouns_from_text(\"Alice sent Bob an encrypted message\") == 'sent an encrypted message'\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "Using your new function, preprocess the dataset and remove all proper nouns.\n",
    "Then, recompute the TF-IDF matrix and the K-Means clustering.\n",
    "As a result, your clustering score should increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "pos_filtered_synopses = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "tfidf_vectorizer = \n",
    "\n",
    "tfidf_matrix = \n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 7\n",
    "\n",
    "# initialize clustering\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "# compute K-Means clustering on TF-IDF representation\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "# contains the cluster assignments for each observation\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "# put the results of the clustering to a dataframe\n",
    "films = { 'title': titles, 'rank': list(range(len(synopses))), \n",
    "         'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
    "frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'title', 'cluster', 'genre'])\n",
    "\n",
    "# compute the clustering score (bigger is better)\n",
    "new_score = km.score(tfidf_matrix)\n",
    "print('New clustering score: {}'.format(new_score))\n",
    "assert new_score > -85\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_words = printout_clusters(km, num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### This looks better, but there is more we can do here. To improve the quality of our representation, we could remove the very frequent and very infrequent words from our TF-IDF matrix. This should make the clusters more diverse and distinguishable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2.4\n",
    "Experiment with tfidf_vectorizer parameters min_df and max_df to downsample most frequent and least frequent terms.\n",
    "Find the parameters that give a clustering score > -70, but leave at least 500 features for your representationn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "tfidf_vectorizer = \n",
    "\n",
    "tfidf_matrix = \n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "print(\"Features left: {}\".format(len(terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 7\n",
    "# initialize clustering\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "# compute K-Means clustering on TF-IDF representation\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "# contains the cluster assignments for each observation\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "# put the results of the clustering to a dataframe\n",
    "films = { 'title': titles, 'rank': list(range(len(synopses))), \n",
    "         'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
    "frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'title', 'cluster', 'genre'])\n",
    "\n",
    "# compute the clustering score (bigger is better)\n",
    "new_score = km.score(tfidf_matrix)\n",
    "print('New clustering score: {}'.format(new_score))\n",
    "assert new_score > -70\n",
    "assert len(terms) > 500\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_words = printout_clusters(km, num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing document clusters with multidimensional scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os  # for os.path.basename\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "MDS()\n",
    "\n",
    "# two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {i: np.random.rand(3,) for i in range(num_clusters)}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = cluster_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(16, 16)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "            label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelleft='off')\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.loc[i]['x'], df.loc[i]['y'], df.loc[i]['title'], size=8)  \n",
    "\n",
    "    \n",
    "plt.show() #show the plot\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vis_utils import css, TopToolbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "#define custom css to format the font and to remove the axis labeling\n",
    "\n",
    "\n",
    "# Plot \n",
    "fig, ax = plt.subplots(figsize=(14,6)) #set plot size\n",
    "ax.margins(0.03) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=18, label=cluster_names[name], mec='none', color=cluster_colors[name])\n",
    "    ax.set_aspect('auto')\n",
    "    labels = [i for i in group.title]\n",
    "    \n",
    "    #set tooltip using points, labels and the already defined 'css'\n",
    "    tooltip = mpld3.plugins.PointHTMLTooltip(points[0], labels,\n",
    "                                       voffset=10, hoffset=10, css=css)\n",
    "    #connect tooltip to fig\n",
    "    mpld3.plugins.connect(fig, tooltip, TopToolbar())    \n",
    "    \n",
    "    #set tick marks as blank\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    #set axis as blank\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    \n",
    "ax.legend(numpoints=1) #show legend with only one dot\n",
    "\n",
    "mpld3.display() #show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical document clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=titles);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "#uncomment below to save figure\n",
    "plt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
