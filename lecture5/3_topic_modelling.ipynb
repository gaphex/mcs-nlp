{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lda.datasets.load_reuters()\n",
    "print(\"type(X): {}\".format(type(X)))\n",
    "print(\"shape: {}\\n\".format(X.shape))\n",
    "\n",
    "# the vocab\n",
    "vocab = lda.datasets.load_reuters_vocab()\n",
    "print(\"type(vocab): {}\".format(type(vocab)))\n",
    "print(\"len(vocab): {}\\n\".format(len(vocab)))\n",
    "\n",
    "# titles for each story\n",
    "titles = lda.datasets.load_reuters_titles()\n",
    "print(\"type(titles): {}\".format(type(titles)))\n",
    "print(\"len(titles): {}\\n\".format(len(titles)))\n",
    "\n",
    "id_to_word = dict(zip(range(len(vocab)), vocab))\n",
    "word_to_id = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our data is a count-vectorized matrix similar to TF-IDF (but without the word weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "word_id = 3117\n",
    "\n",
    "print(\"doc id: {} word id: {}\".format(doc_id, word_id))\n",
    "print(\"-- count: {}\".format(X[doc_id, word_id]))\n",
    "print(\"-- word : {}\".format(vocab[word_id]))\n",
    "print(\"-- title  : {}\".format(titles[doc_id]))\n",
    "print(\"-- words : {}\".format([id_to_word[t] for t in X[doc_id].nonzero()[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = 20\n",
    "model = lda.LDA(n_topics=N_TOPICS, n_iter=500, random_state=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The topic model for each word outputs a probability of it belonging to one of the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_distribution = model.topic_word_\n",
    "print(\"type(topic_word): {}\".format(type(topic_word_distribution)))\n",
    "print(\"shape: {}\".format(topic_word_distribution.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The matrix rows are a valid probability distribution (all probs sum to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(topic_word_distribution, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "Write a function, that for a given topic number 't', outputs 'k' top words (by probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_k_words(topic_word_prob_distribution, topic_number, top_k):\n",
    "    # your code goes here\n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(get_top_k_words(topic_word_distribution, 3, 5)) == ['yeltsin', 'russian', \n",
    "                                                                'russia', 'president', 'kremlin']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N_TOPICS):\n",
    "    topic_words = get_top_k_words(topic_word_distribution, i, 10)\n",
    "    print('*Topic {}\\n- {}'.format(i+1, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The topic model for each document outputs a probability of it belonging to one of the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "print(\"type(doc_topic): {}\".format(type(doc_topic)))\n",
    "print(\"shape: {}\".format(doc_topic.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(doc_topic, axis=1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform(X[:3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(10):\n",
    "    topic_most_pr = doc_topic[n].argmax()\n",
    "    print(\"doc: {} topic: {}\\n{}...\".format(n,\n",
    "                                            topic_most_pr,\n",
    "                                            titles[n]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
