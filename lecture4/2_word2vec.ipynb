{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import random\n",
    "import gensim\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec: skip gram & cbow\n",
    "\n",
    "Models __CBOW (Continuous Bag of Words)__ and __Skip gram__ were invented in the now distant 2013,\n",
    "*article*:\n",
    "[*Tomas Mikolov et al.*](https://arxiv.org/pdf/1301.3781v3.pdf)\n",
    "\n",
    "* __CBOW__ model predict missing word (focus word) using context (surrounding words).\n",
    "* __skip gram__ model is reverse to _CBOW_. It predicts context based on the word in focus.\n",
    "\n",
    "* **Context** is a fixed number of words to the left and right of the word in focus (see picture below). The length of the context is defined by the \"window\" parameter.\n",
    "\n",
    "![context](pics/context.png)\n",
    "\n",
    "Two models comparision\n",
    "\n",
    "![architecture](pics/architecture.png)\n",
    "\n",
    "\n",
    "### Skip_gram\n",
    "\n",
    "Consider a corpus with a sequence of words $ w_1, w_2, .., w_T $.\n",
    "\n",
    "Objective function (we would like to maximize it) for _skip gram_ is defined as follow:\n",
    "\n",
    "\n",
    "$$ AverageLogProbability = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leqslant j\\leqslant c, j \\neq 0} log\\ p (w_{t+j} | w_t) $$\n",
    "\n",
    "* where $ c $ is a context length.\n",
    "* $w_t$ -- focus word\n",
    "\n",
    "The basic formulation for probability $ p (w_{t+j} | w_t) $ is calculated using __Softmax__ -\n",
    "\n",
    "$$ p (w_h | w_i) = \\frac{exp(s(v_i, v_h))}{ \\sum^{W}_{w=1}  exp(s(v_{w}, v_{i} )) } $$\n",
    "\n",
    "where\n",
    "* $w_i$ -- input focus word\n",
    "* $w_h$ -- hypothetically context word for a given focus word $w_i$\n",
    "* $v_i$ and $v_h$ input-word and hypothesis-word vector representations (for $w_i$, $w_h$)\n",
    "* $s(v_i, v_h) = v^{T} _{h} \\cdot v_{i}$\n",
    "* $W$ is the number of words in vocabulary\n",
    "\n",
    "___\n",
    "\n",
    "### CBOW\n",
    "\n",
    "Predict word using context.\n",
    "\n",
    "$$ E = -log\\ p(w_h\\ |\\ w_{1},\\ w_{2},\\ \\dots,\\ w_{c}) $$\n",
    "\n",
    "\n",
    "The **probability** is the same as in the *skip gram* model, but now $v_i$ is a sum of context-word vectors.\n",
    "\n",
    "$$ p(w_h\\ |\\ w_{1},\\ w_{2},\\ \\dots,\\ w_{c})  = \\frac{exp(s(v_i, v_h))}{\\sum^{W}_{w=1}  exp(s(v_{w}, v_{i}))} $$\n",
    "\n",
    "\n",
    "* $\\ w_{1},\\ w_{2},\\ \\dots,\\ w_{c}$ -- input context words\n",
    "* $w_h$ -- hypothetically focus word for a given context words\n",
    "* $ v_i = \\sum^{c}_{k=1} w_{k}$\n",
    "* $ v_h$ = vector of hypothesis word\n",
    "* $s(v_i, v_h) = v^{T} _{h} \\cdot v_{i}$\n",
    "* $W$ is the number of words in vocabulary\n",
    "\n",
    "___\n",
    "\n",
    "Lets implement __`CBOW`__ using tensorflow framework.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 2: preparing the data, building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you prepare the data for the word2ve neural network by tokenizing it, building a dictionary and encoding it with corresponding identifiers. \n",
    "\n",
    "You may want to use the functions you have written during Seminar 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.dropbox.com/s/klednu69tgfap8n/train.csv?dl=1'\n",
    "\n",
    "def maybe_download(dl_url, filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        u = urllib.request.urlopen(dl_url)\n",
    "        data = u.read()\n",
    "        u.close()\n",
    "        with open(filename, \"wb\") as f :\n",
    "            f.write(data)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "filename = maybe_download(url, 'train.csv', 101536537) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the texts from csv file and convert them into a single list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_texts = csv['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "all_texts_joint = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "# use any tokenizer you deem necessary\n",
    "tokens = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert isinstance(tokens, list)\n",
    "assert isinstance(tokens[0], str)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dictionary {token: token_id} of 50000 most frequent tokens.\n",
    "\n",
    "After that, make an inverse dictionary {token_id: token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens, max_size=20000):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary of at most max_size words from the supplied list of lists of tokens.\n",
    "    If a word embedding model is provided, adds only the words present in the model vocabulary.\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = build_vocabulary(tokens, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "reverse_dictionary = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(dictionary) == len(reverse_dictionary) == 50000\n",
    "assert sorted(dictionary.keys()) == sorted(reverse_dictionary.values())\n",
    "assert sorted(reverse_dictionary.keys()) == sorted(dictionary.values())\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the tokens into a list of their identifiers from your 'dictionary'. Replace the Out Of Vocabulary [OOV] tokens with 'UNKN' identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(tokens, token_to_id):\n",
    "    # your code goes here\n",
    "    return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = encode(tokens, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Sample encoded data', data[:5])\n",
    "print('Sample decoded data', [reverse_dictionary[t] for t in data[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t, tid in zip(tokens, data):\n",
    "    assert ((reverse_dictionary[tid] == t) or (tid==dictionary['UNKN']))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are done with data preparation, now train the word2vec model. You don't need to change anything in this section.\n",
    "\n",
    "Read the comments to get a better understanding of what is happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for num_skips, skip_window in [(2, 1)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow word2vec computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "    # This is because the embeddings are defined as a variable quantity and the\n",
    "    # optimizer's `minimize` method will by default modify all variable quantities \n",
    "    # that contribute to the tensor it is passed.\n",
    "    # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 100001\n",
    "lh = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_batch(\n",
    "          batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            display.clear_output(wait=True)\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            lh.append(average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "            plt.figure(figsize=(14, 10))\n",
    "\n",
    "            plt.title(\"Training loss, step {}/{}\".format(step, num_steps))\n",
    "            plt.xlabel(\"#step\")\n",
    "            plt.ylabel(\"loss\")\n",
    "            plt.plot(lh, 'b')\n",
    "            plt.show()\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print(log)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the word vectors and dictionaries for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vec_file(final_emb_mtx, vocab_size, vec_size,filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(str(vocab_size)+' '+str(vec_size) + '\\n')\n",
    "        for n in range(vocab_size):\n",
    "            s = ' '.join([reverse_dictionary[n]] + [str(num) for num in final_emb_mtx[n]])\n",
    "            f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_vec_file(final_embeddings, vocab_size=vocabulary_size, vec_size=embedding_size, filename='simple_cbow.w2v')\n",
    "pickle.dump([dictionary, reverse_dictionary], open('dict_rdict.pkl',\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T_SNE projection of word vectors into a 2-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd = json.load(open('topical_words.json'))\n",
    "colors = {'colors':'r', 'vegetables':'g', 'numbers':'b', 'professions': 'GoldenRod'}\n",
    "wd = {}\n",
    "for k in dd:\n",
    "    for w in dd[k]:\n",
    "        if w in dictionary:\n",
    "            wd[w] = {'emb': final_embeddings[dictionary[w]], 'label': k, 'color': colors[k]}\n",
    "            \n",
    "M = np.array([wd[k]['emb'] for k in wd])\n",
    "labels = [wd[k]['label'] for k in wd]\n",
    "colors = [wd[k]['color'] for k in wd]\n",
    "\n",
    "P = squareform(pdist(M, metric='cosine'))\n",
    "tsne2 = TSNE(n_components=2, random_state=34, metric='precomputed', n_iter=9001)\n",
    "Y = tsne2.fit_transform(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i, k in enumerate(wd):\n",
    "    plt.scatter([Y[i][0]], [Y[i][1]], color = wd[k]['color'], label=wd[k]['label'])\n",
    "    plt.annotate(k, xy=(Y[i][0], Y[i][1]), xytext=(0, 0), textcoords='offset points', \n",
    "                 color=wd[k]['color'], fontsize=12)\n",
    "\n",
    "\n",
    "plt.title('T-SNE on word2vec representations, 4 topics', fontsize=16)        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
