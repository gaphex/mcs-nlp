{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_data = pd.read_csv(\"./train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_TO_USE = 250000\n",
    "\n",
    "texts = sentiment_data['message'].tolist()[:DATA_TO_USE]\n",
    "labels = np.array(sentiment_data['sentiment'])[:DATA_TO_USE]\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: word vectors meet bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you use your newly trained word vectors and a simple Bag of Words models to approach the sentiment analysis task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will use a convinient wrapper for our word2vec model provided by gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format(\"./simple_cbow.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0688487 , -0.169258  , -0.0235873 ,  0.0768677 , -0.0560077 ,\n",
       "       -0.20571201,  0.0722242 , -0.0238068 ,  0.0942261 ,  0.0445618 ,\n",
       "       -0.0930745 , -0.132294  , -0.0380145 , -0.0885391 ,  0.0289901 ,\n",
       "        0.0770231 , -0.122311  , -0.0118321 ,  0.13177601, -0.0227252 ,\n",
       "        0.16474199, -0.0572786 ,  0.0745815 , -0.0760634 ,  0.0295188 ,\n",
       "       -0.0441017 , -0.0895143 , -0.0566907 ,  0.0281097 , -0.00958456,\n",
       "       -0.110452  ,  0.0415194 , -0.0153895 , -0.0745323 ,  0.00762951,\n",
       "       -0.0319146 , -0.0557301 , -0.0886996 ,  0.0806743 , -0.0471586 ,\n",
       "       -0.0859856 , -0.20486601, -0.118638  ,  0.111954  ,  0.0344212 ,\n",
       "       -0.0233198 ,  0.0737764 ,  0.0346345 , -0.0558539 , -0.211096  ,\n",
       "       -0.0435312 ,  0.115727  , -0.0596598 , -0.0322688 , -0.0538118 ,\n",
       "        0.0722077 ,  0.0208389 , -0.15472899,  0.0143741 , -0.126031  ,\n",
       "        0.0622076 ,  0.032778  ,  0.0305527 ,  0.0810955 ,  0.010537  ,\n",
       "        0.0163742 , -0.0827605 , -0.0131961 ,  0.0289909 , -0.0646999 ,\n",
       "       -0.0708455 ,  0.0939854 ,  0.166179  ,  0.155966  , -0.0936102 ,\n",
       "        0.00879914,  0.0360047 ,  0.0902226 , -0.0187742 , -0.0600839 ,\n",
       "        0.0391964 , -0.0975536 , -0.0356622 , -0.0324537 ,  0.104185  ,\n",
       "        0.130004  , -0.13667101, -0.104945  , -0.0836355 ,  0.0663564 ,\n",
       "        0.0790161 , -0.0118584 ,  0.00110292, -0.075322  ,  0.0269967 ,\n",
       "       -0.0704023 , -0.0582241 ,  0.0780745 ,  0.0583372 , -0.116921  ,\n",
       "       -0.0298988 , -0.0849007 ,  0.13700899, -0.0846834 , -0.12925801,\n",
       "        0.00485594,  0.063532  , -0.0317335 ,  0.102577  , -0.0772568 ,\n",
       "       -0.17413101, -0.0858324 , -0.13659699, -0.00087295,  0.00115875,\n",
       "       -0.0277262 ,  0.0650055 , -0.0741444 ,  0.05642   , -0.126343  ,\n",
       "       -0.0656122 ,  0.203464  , -0.0750102 ,  0.0168465 , -0.0828873 ,\n",
       "       -0.135879  ,  0.0356199 ,  0.22182199], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can get the vector for a word in a simple way\n",
    "w2v_model['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cool', 0.5530734062194824),\n",
       " ('good', 0.4626672565937042),\n",
       " ('nice', 0.44066357612609863),\n",
       " ('awesome', 0.438425213098526),\n",
       " ('exciting', 0.43373095989227295),\n",
       " ('weird', 0.43178021907806396),\n",
       " ('cute', 0.40261897444725037),\n",
       " ('interesting', 0.3848278522491455),\n",
       " ('awesome!', 0.3723982572555542),\n",
       " ('amazing', 0.36828142404556274)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can easily query the model for word most similar to a give word \n",
    "w2v_model.most_similar('funny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you learn how to encode sentences with word2vec using a bag of words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement a tokenizer that you will use throughout the exercise\n",
    "# I would recommend a regexp tokenizer for speed, but it's completely up to you\n",
    "def my_tokenizer(text):\n",
    "    return nltk.regexp_tokenize(text, '\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_encoder(wmodel, tokenizer, text):\n",
    "    \"\"\"\n",
    "    This function encodes text into a vector.\n",
    "    \n",
    "    First, it tokenizes input text using the provided tokenizer function.\n",
    "    Then it uses the provided word2vec model to get the vectors corresponding to text's tokens.\n",
    "    Finally, it computes an average of all token's vectors and returns it.\n",
    "    \n",
    "    If the function failed to find and encode any words, it should at least return a vector of zeros.\n",
    "    \"\"\"\n",
    "    zero_vector = np.zeros(w2v_model.vector_size)\n",
    "    # your code goes here\n",
    "    return sent_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your new encoder to encode both train_texts and test_texts into matrices.\n",
    "\n",
    "The number of rows in a matrix should be equal to the number of texts encoded.\n",
    "\n",
    "The number of columns should be equal to the word2vec space dimansionality (currently = 128)\n",
    "\n",
    "Just write a little loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187500/187500 [00:18<00:00, 9957.46it/s] \n",
      "100%|██████████| 62500/62500 [00:06<00:00, 10124.50it/s]\n"
     ]
    }
   ],
   "source": [
    "train_encoded = \n",
    "test_encoded = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(train_encoded, np.ndarray)\n",
    "assert isinstance(test_encoded, np.ndarray)\n",
    "\n",
    "assert train_encoded.shape[0] == len(train_texts)\n",
    "assert train_encoded.shape[1] == w2v_model.vector_size\n",
    "\n",
    "assert test_encoded.shape[0] == len(test_texts)\n",
    "assert test_encoded.shape[1] == w2v_model.vector_size\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(train_encoded, train_labels)\n",
    "preds = clf.predict(test_encoded)\n",
    "\n",
    "print(classification_report(test_labels, preds))\n",
    "print(\"AUC = {}\".format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not too impressive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you attempt to improve your encoder by filtering out stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_encoder_with_stopwords(wmodel, tokenizer, stopwords, text):\n",
    "    \"\"\"\n",
    "    This function encodes text into a vector.\n",
    "    \n",
    "    First, it tokenizes input text using the provided tokenizer function.\n",
    "    Then it removes any stopwords from the list of tokens.\n",
    "    Then it uses the provided word2vec model to get the vectors corresponding to text's tokens.\n",
    "    Finally, it computes an average of all token's vectors and returns it.\n",
    "    \n",
    "    If the function failed to find and encode any words, it should at least return a vector of zeros.\n",
    "    \"\"\"\n",
    "    zero_vector = np.zeros(w2v_model.vector_size)\n",
    "    # your code goes here\n",
    "    return sent_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187500/187500 [00:16<00:00, 11398.73it/s]\n",
      "100%|██████████| 62500/62500 [00:05<00:00, 12188.47it/s]\n"
     ]
    }
   ],
   "source": [
    "train_encoded = \n",
    "test_encoded = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(train_encoded, train_labels)\n",
    "preds = clf.predict(test_encoded)\n",
    "\n",
    "print(classification_report(test_labels, preds))\n",
    "print(\"AUC = {}\".format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looks like the BoW model is not too good for the job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![architecture](pics/we_need_to_go_deeper.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing: Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a cool library built on top of the computational backend provided by Tensorflow. It provides a layer of abstraction between you and complicated tensor algebra, allowing for rapid prototyping of deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.1: Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start crunching word vectors with convolutional neural networks, we need to prepare our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the vocabulary we created earlier\n",
    "voc, rvoc = pickle.load(open(\"./dict_rdict.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we are going to use the whole dataset this time around\n",
    "texts = sentiment_data['message'].tolist()\n",
    "labels = np.array(sentiment_data['sentiment'])\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the function that turns tokens into their ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hint: you may want to use the function you've built during seminar 2\n",
    "MAX_LEN = 32\n",
    "\n",
    "def vectorize_tokens(sentence, tokenizer, token_to_id, max_len):\n",
    "    \"\"\"\n",
    "    Preprocesses a sentence into list of tokens using the provided tokenizer\n",
    "    Then converts it into a list of token ids using the supplied 'token_to_id' dictionary.\n",
    "    Pads resulting list with NULL identifiers up to max_len length. \n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "    # STEP 1: convert sentence to a list of tokens\n",
    "    \n",
    "    # STEP 2: replace tokens with their identifiers from the vocabulary\n",
    "    # If the token is not present in the vocabulary, replace it with UNKN identifier\n",
    "\n",
    "    # STEP 3: pad the sequence id's with NULL identifiers until so that it's length is equal to max_len\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the vectorization function to every sentence from train and test datasets. In the end you should end up with a matrix of shape [len(data), MAX_LEN].\n",
    "\n",
    "Just write a little loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_sentences(sentences, tokenizer, token_to_id, max_len):\n",
    "    sentence_ids = []\n",
    "    \n",
    "    # your code goes here\n",
    "        \n",
    "    return np.array(sentence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectorized = vectorize_sentences(train_texts, my_tokenizer, voc, MAX_LEN)\n",
    "test_vectorized = vectorize_sentences(test_texts, my_tokenizer, voc, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(train_vectorized, np.ndarray)\n",
    "assert isinstance(test_vectorized, np.ndarray)\n",
    "\n",
    "assert train_vectorized.shape == (len(train_vectorized), MAX_LEN)\n",
    "assert test_vectorized.shape == (len(test_vectorized), MAX_LEN)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Building a deep NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = w2v_model.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras Input layer is basically the same thing as tf.placeholder\n",
    "# it defines a node where the network will be expecting to recieve input data\n",
    "input_layer = keras.layers.Input(shape=(MAX_LEN,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras Embedding layer is a container for dense vectors\n",
    "# it recieves a list of token identifiers of shape [MAX_LEN] \n",
    "# and turns it into a matrix of shape [MAX_LEN, EMBEDDING_DIM]\n",
    "\n",
    "embedding_layer = keras.layers.Embedding(embeddings_matrix.shape[0], embeddings_matrix.shape[1], \n",
    "                                         input_length=MAX_LEN, weights=[embeddings_matrix],\n",
    "                                         trainable=False)(input_layer)\n",
    "# notice how the input_layer is plugged into the embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras Convolutional layer implements a set of learnable filters\n",
    "# that extract local patterns from input data\n",
    "convolution_layer = keras.layers.Convolution1D(128, 3)(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras GlobalMaxPooling layer applies a max filter to the input feature representation\n",
    "# only the strongest responses from the previous layer are kept, everything else is discarded\n",
    "subsampling_layer = keras.layers.GlobalMaxPooling1D()(convolution_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras Linear layers apply a simple linear transformation to input data, \n",
    "# which is optionally followed by a non-linear activation function\n",
    "# very useful for building Multi-Layer Perceptrons\n",
    "linear_layer_1 = keras.layers.Dense(64, activation='relu')(subsampling_layer)\n",
    "linear_layer_2 = keras.layers.Dense(1, activation='sigmoid')(linear_layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this compiles the computational graph we've just created, applies a loss function\n",
    "# and pre-computes the gradients for back propagation\n",
    "\n",
    "deep_model = keras.models.Model(inputs=[input_layer], outputs=[linear_layer_2])\n",
    "deep_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 32, 128)           6400000   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 30, 128)           49280     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 6,457,601\n",
      "Trainable params: 57,601\n",
      "Non-trainable params: 6,400,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 937500 samples, validate on 312500 samples\n",
      "Epoch 1/10\n",
      "937500/937500 [==============================] - 687s 733us/step - loss: 0.5105 - acc: 0.7457 - val_loss: 0.4829 - val_acc: 0.7653\n",
      "Epoch 2/10\n",
      "937500/937500 [==============================] - 670s 714us/step - loss: 0.4673 - acc: 0.7761 - val_loss: 0.4713 - val_acc: 0.7739\n",
      "Epoch 3/10\n",
      "937500/937500 [==============================] - 721s 769us/step - loss: 0.4516 - acc: 0.7860 - val_loss: 0.4628 - val_acc: 0.7785\n",
      "Epoch 4/10\n",
      "937500/937500 [==============================] - 684s 729us/step - loss: 0.4409 - acc: 0.7926 - val_loss: 0.4625 - val_acc: 0.7798\n",
      "Epoch 5/10\n",
      "937500/937500 [==============================] - 668s 713us/step - loss: 0.4334 - acc: 0.7973 - val_loss: 0.4632 - val_acc: 0.7802\n",
      "Epoch 6/10\n",
      "937500/937500 [==============================] - 688s 734us/step - loss: 0.4272 - acc: 0.8012 - val_loss: 0.4584 - val_acc: 0.7821\n",
      "Epoch 7/10\n",
      "937500/937500 [==============================] - 725s 773us/step - loss: 0.4222 - acc: 0.8038 - val_loss: 0.4570 - val_acc: 0.7829\n",
      "Epoch 8/10\n",
      "937500/937500 [==============================] - 659s 703us/step - loss: 0.4183 - acc: 0.8063 - val_loss: 0.4640 - val_acc: 0.7825\n",
      "Epoch 9/10\n",
      "937500/937500 [==============================] - 666s 711us/step - loss: 0.4145 - acc: 0.8083 - val_loss: 0.4589 - val_acc: 0.7827\n",
      "Epoch 10/10\n",
      "937500/937500 [==============================] - 710s 757us/step - loss: 0.4114 - acc: 0.8104 - val_loss: 0.4594 - val_acc: 0.7828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122341860>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model.fit(x=train_vectorized, y=train_labels, batch_size=64, epochs=3, \n",
    "               validation_data=[test_vectorized, test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = deep_model.predict(test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.865941180432684\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC = {}\".format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thats more like it! Keep in mind that we only trained a tiny (57k parameters) model because of the limitations of CPU computing power. Using a deeper model with more trainable filters in the Convolution layer would likely result in even stronger predictive power. Stay tuned! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
