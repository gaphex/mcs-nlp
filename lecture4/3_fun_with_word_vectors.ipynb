{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_data = pd.read_csv(\"./train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_TO_USE = 250000\n",
    "\n",
    "texts = sentiment_data['message'].tolist()[:DATA_TO_USE]\n",
    "labels = np.array(sentiment_data['sentiment'])[:DATA_TO_USE]\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: word vectors meet bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you use your newly trained word vectors and a simple Bag of Words models to approach the sentiment analysis task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will use a convinient wrapper for our word2vec model provided by gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format(\"./simple_cbow.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can get the vector for a word in a simple way\n",
    "w2v_model['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can easily query the model for word most similar to a give word \n",
    "w2v_model.most_similar('funny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you learn how to encode sentences with word2vec using a bag of words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement a tokenizer that you will use throughout the exercise\n",
    "# I would recommend a regexp tokenizer for speed, but it's completely up to you\n",
    "def my_tokenizer(text):\n",
    "    return nltk.regexp_tokenize(text, '\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_encoder(wmodel, tokenizer, text):\n",
    "    \"\"\"\n",
    "    This function encodes text into a vector.\n",
    "    \n",
    "    First, it tokenizes input text using the provided tokenizer function.\n",
    "    Then it uses the provided word2vec model to get the vectors corresponding to text's tokens.\n",
    "    Finally, it computes an average of all token's vectors and returns it.\n",
    "    \n",
    "    If the function failed to find and encode any words, it should at least return a vector of zeros.\n",
    "    \"\"\"\n",
    "    zero_vector = np.zeros(w2v_model.vector_size)\n",
    "    # your code goes here\n",
    "    return sent_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your new encoder to encode both train_texts and test_texts into matrices.\n",
    "\n",
    "The number of rows in a matrix should be equal to the number of texts encoded.\n",
    "\n",
    "The number of columns should be equal to the word2vec space dimansionality (currently = 128)\n",
    "\n",
    "Just write a little loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_encoded = \n",
    "test_encoded = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert isinstance(train_encoded, np.ndarray)\n",
    "assert isinstance(test_encoded, np.ndarray)\n",
    "\n",
    "assert train_encoded.shape[0] == len(train_texts)\n",
    "assert train_encoded.shape[1] == w2v_model.vector_size\n",
    "\n",
    "assert test_encoded.shape[0] == len(test_texts)\n",
    "assert test_encoded.shape[1] == w2v_model.vector_size\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(train_encoded, train_labels)\n",
    "preds = clf.predict(test_encoded)\n",
    "\n",
    "print(classification_report(test_labels, preds))\n",
    "print(\"AUC = {}\".format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not too impressive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you attempt to improve your encoder by filtering out stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_encoder_with_stopwords(wmodel, tokenizer, stopwords, text):\n",
    "    \"\"\"\n",
    "    This function encodes text into a vector.\n",
    "    \n",
    "    First, it tokenizes input text using the provided tokenizer function.\n",
    "    Then it removes any stopwords from the list of tokens.\n",
    "    Then it uses the provided word2vec model to get the vectors corresponding to text's tokens.\n",
    "    Finally, it computes an average of all token's vectors and returns it.\n",
    "    \n",
    "    If the function failed to find and encode any words, it should at least return a vector of zeros.\n",
    "    \"\"\"\n",
    "    zero_vector = np.zeros(w2v_model.vector_size)\n",
    "    # your code goes here\n",
    "    return sent_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_encoded = \n",
    "test_encoded = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(train_encoded, train_labels)\n",
    "preds = clf.predict(test_encoded)\n",
    "\n",
    "print(classification_report(test_labels, preds))\n",
    "print(\"AUC = {}\".format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looks like the BoW model is not too good for the job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![architecture](pics/we_need_to_go_deeper.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing: Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a cool library built on top of the computational backend provided by Tensorflow. It provides a layer of abstraction between you and complicated tensor algebra, allowing for rapid prototyping of deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.3: Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start crunching word vectors with convolutional neural networks, we need to prepare our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the vocabulary we created earlier\n",
    "voc, rvoc = pickle.load(open(\"./dict_rdict.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we are going to use the whole dataset this time around\n",
    "texts = sentiment_data['message'].tolist()\n",
    "labels = np.array(sentiment_data['sentiment'])\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the function that turns tokens into their ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hint: you may want to use the function you've built during seminar 2\n",
    "MAX_LEN = 32\n",
    "\n",
    "def vectorize_tokens(sentence, tokenizer, token_to_id, max_len):\n",
    "    \"\"\"\n",
    "    Preprocesses a sentence into list of tokens using the provided tokenizer\n",
    "    Then converts it into a list of token ids using the supplied 'token_to_id' dictionary.\n",
    "    Pads resulting list with NULL identifiers up to max_len length. \n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "    # STEP 1: convert sentence to a list of tokens\n",
    "    \n",
    "    # STEP 2: replace tokens with their identifiers from the vocabulary\n",
    "    # If the token is not present in the vocabulary, replace it with UNKN identifier\n",
    "\n",
    "    # STEP 3: pad the sequence id's with NULL identifiers until so that it's length is equal to max_len\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the vectorization function to every sentence from train and test datasets. In the end you should end up with a matrix of shape [len(data), MAX_LEN].\n",
    "\n",
    "Just write a little loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_sentences(sentences, tokenizer, token_to_id, max_len):\n",
    "    sentence_ids = []\n",
    "    \n",
    "    # your code goes here\n",
    "        \n",
    "    return np.array(sentence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vectorized = vectorize_sentences(train_texts, my_tokenizer, voc, MAX_LEN)\n",
    "test_vectorized = vectorize_sentences(test_texts, my_tokenizer, voc, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert isinstance(train_vectorized, np.ndarray)\n",
    "assert isinstance(test_vectorized, np.ndarray)\n",
    "\n",
    "assert train_vectorized.shape == (len(train_vectorized), MAX_LEN)\n",
    "assert test_vectorized.shape == (len(test_vectorized), MAX_LEN)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Building a deep NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_matrix = w2v_model.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras Input layer is basically the same thing as tf.placeholder\n",
    "# it defines a node where the network will be expecting to recieve input data\n",
    "input_layer = keras.layers.Input(shape=(MAX_LEN,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras Embedding layer is a container for dense vectors\n",
    "# it recieves a list of token identifiers of shape [MAX_LEN] \n",
    "# and turns it into a matrix of shape [MAX_LEN, EMBEDDING_DIM]\n",
    "\n",
    "embedding_layer = keras.layers.Embedding(embeddings_matrix.shape[0], embeddings_matrix.shape[1], \n",
    "                                         input_length=MAX_LEN, weights=[embeddings_matrix],\n",
    "                                         trainable=False)(input_layer)\n",
    "# notice how the input_layer is plugged into the embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras Convolutional layer implements a set of learnable filters\n",
    "# that extract local patterns from input data\n",
    "convolution_layer = keras.layers.Convolution1D(128, 3)(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras GlobalMaxPooling layer applies a max filter to the input feature representation\n",
    "# only the strongest responses from the previous layer are kept, everything else is discarded\n",
    "subsampling_layer = keras.layers.GlobalMaxPooling1D()(convolution_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras Linear layers apply a simple linear transformation to input data, \n",
    "# which is optionally followed by a non-linear activation function\n",
    "# very useful for building Multi-Layer Perceptrons\n",
    "linear_layer_1 = keras.layers.Dense(64, activation='relu')(subsampling_layer)\n",
    "linear_layer_2 = keras.layers.Dense(1, activation='sigmoid')(linear_layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this compiles the computational graph we've just created, applies a loss function\n",
    "# and pre-computes the gradients for back propagation\n",
    "\n",
    "deep_model = keras.models.Model(inputs=[input_layer], outputs=[linear_layer_2])\n",
    "deep_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deep_model.fit(x=train_vectorized, y=train_labels, batch_size=64, epochs=3, \n",
    "               validation_data=[test_vectorized, test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = deep_model.predict(test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"AUC = {}\".format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deep_model.save_weights(\"nn_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thats more like it! Keep in mind that we only trained a tiny (57k parameters) model because of the limitations of CPU computing power. Using a deeper model with more trainable filters in the Convolution layer would likely result in even stronger predictive power. Stay tuned! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
