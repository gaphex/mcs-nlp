{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: word vector representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import operator\n",
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import LinearSVC\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you define a couple of tokenizers and use them on a toy sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentence_1 = 'The quick brown fox jumps over the lazy dog.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - delimiter tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_by_split(text):\n",
    "    \"\"\"Tokenizes a given string of text by splitting words by whitespace\"\"\"\n",
    "    # your code goes here\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenize_by_split(test_sentence_1) == ['The', 'quick', 'brown', 'fox', \n",
    "                                              'jumps', 'over', 'the', 'lazy', 'dog.']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punkt_and_tokenize_by_split(text):\n",
    "    \"\"\"Replaces punktuation from given string of text with whitespace, then\n",
    "    tokenizes it by splitting words by whitespace\"\"\"\n",
    "    punkt_symbols = string.punctuation\n",
    "    # your code goes here\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert remove_punkt_and_tokenize_by_split(test_sentence_1) == ['The', 'quick', 'brown', 'fox', \n",
    "                                                               'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentence_2 = \"This is a test that isn't so simple: 1.23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_by_regex(text):\n",
    "    \"\"\"Tokenizes a given string of text by applying the 'tokenize' method \n",
    "    of the provided 'tokenizer' object\"\"\"\n",
    "    tokenizer = nltk.RegexpTokenizer('\\w+')\n",
    "    # your code goes here\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenize_by_regex(test_sentence_2) == ['This', 'is', 'a', 'test', 'that', \n",
    "                                              'isn', 't', 'so', 'simple', '1', '23']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - using an advanced tokenizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the PUNKT tokenizer model\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_by_punkt_model(text):\n",
    "    \"\"\"Tokenizes a given string of text by applying the NLTK Punkt tokenizer model.\n",
    "    Uses nltk.word_tokenize method\"\"\"\n",
    "    # your code goes here\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenize_by_punkt_model(test_sentence_2) == ['This', 'is', 'a', 'test', 'that', \n",
    "                                                    'is', \"n't\", 'so', 'simple', ':', '1.23']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: n-grams and stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. First of all, let's get it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown corpus contains texts from different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences from each category can be accessed as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_sents = list(nltk.corpus.brown.sents(categories='adventure'))\n",
    "print(len(adv_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_sents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what are the most frequent unigrams in the 'adventure' category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# joins a list of lists of tokens into a one large string of text\n",
    "adventure_text = ' '.join(list(itertools.chain.from_iterable(adv_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uses the tokenizer function we've just written to tokenize text\n",
    "adventure_tokens = tokenize_by_regex(adventure_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns each token to lowercase (simple normalization technique)\n",
    "lowered_tokens = [token.lower() for token in adventure_tokens]\n",
    "print(len(lowered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# counts the number of occurances for each unigram\n",
    "word_counter = collections.Counter(lowered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [w[0] for w in word_counter.most_common(35)]\n",
    "values = [w[1] for w in word_counter.most_common(35)]\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(indexes, values)\n",
    "plt.xticks(indexes, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that some of the most common words above are not very interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exersise you remove stopwords, find the most frequent bigrams, then display them on a barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of english stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - filtering stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove stopwords from the list of 'lowered_tokens'\n",
    "# your code goes here\n",
    "stopword_filtered_tokens = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - getting the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn your filtered list of unigrams into a list of bigrams, joint by whitespace\n",
    "# to achieve that, use the function nltk.ngrams(your_tokens, 2)\n",
    "# your code goes here\n",
    "filtered_bigrams = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - counting occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now count the occurances of bigrams using a new Counter instance\n",
    "# your code goes here\n",
    "bigram_counter = \n",
    "\n",
    "assert {'miss langford', 'mary jane', 'billy tilghman'}.issubset(set(map(operator.itemgetter(0), \n",
    "                                                                         bigram_counter.most_common(15))))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [w[0] for w in bigram_counter.most_common(15)]\n",
    "values = [w[1] for w in bigram_counter.most_common(15)]\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(indexes, values)\n",
    "plt.xticks(indexes, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you write a function that builds a vocabulary from the provided text corpus. Then you use it to encode tokens into numeric form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - building a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens, max_size):\n",
    "    \"\"\"\n",
    "    Builds a dictionary of at most max_size most frequent tokens from the supplied list of tokens.\n",
    "    More frequent tokens should have a lower id, but that is not strictly required.\n",
    "    Two special symbols 'NULL':0 and 'UNKN':1 should also be added to the dictionary.\n",
    "    \n",
    "    EXAMPLE:\n",
    "    {\n",
    "        'NULL': 0,\n",
    "        'UNKN': 1,\n",
    "        'the': 2,\n",
    "        'and': 3,\n",
    "        'a': 4,\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    vocabulary = {}\n",
    "    reserved_symbols = [\"NULL\", \"UNKN\"]\n",
    "    \n",
    "    # your code goes here\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_SIZE = 5000\n",
    "\n",
    "my_vocabulary = build_vocabulary(lowered_tokens, VOC_SIZE)\n",
    "\n",
    "assert len(my_vocabulary) == VOC_SIZE\n",
    "assert {'NULL', 'UNKN'}.issubset(set(my_vocabulary.keys()))\n",
    "assert set([w[0] for w in word_counter.most_common(VOC_SIZE-10)]).issubset(set(my_vocabulary.keys()))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - encoding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_tokens(sentence, tokenizer, token_to_id, max_len):\n",
    "    \"\"\"\n",
    "    Converts a list of tokens to a list of token ids using the supplied dictionary.\n",
    "    Pads resulting list with NULL identifiers up to max_len length.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    \n",
    "    # STEP 1: convert sentence to a list of tokens\n",
    "    # your code goes here\n",
    "    \n",
    "    # STEP 2: replace tokens with their identifiers from the vocabulary\n",
    "    # If the token is not present in the vocabulary, replace it with UNKN identifier\n",
    "\n",
    "    # STEP 3: pad the sequence id's with NULL identifiers until so that it's length is equal to max_len\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 16\n",
    "test_sentence = 'The animals thundered away into the moonlight , heading for the ridges .'\n",
    "vectorized = vectorize_tokens(test_sentence,\n",
    "                              tokenize_by_regex, my_vocabulary, MAX_LEN)\n",
    "\n",
    "assert len(vectorized) == MAX_LEN\n",
    "assert [my_vocabulary.get(t, my_vocabulary['UNKN']) \n",
    "        for t in tokenize_by_regex(test_sentence)] + [0]*(MAX_LEN-len(tokenize_by_regex(test_sentence))) == vectorized\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you create a function to compute sentence similarity, then build a simple Information Retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_SIZE = 5000\n",
    "\n",
    "adv_brown_sents = [' '.join(sent) for sent in nltk.corpus.brown.sents(categories='adventure')]\n",
    "print(len(adv_brown_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the CountVectorizer instance\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=VOC_SIZE, stop_words=stopwords, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builds the vocabulary from the data\n",
    "tfidf_vectorizer.fit(adv_brown_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer_vocab = dict(zip(range(len(tfidf_vectorizer.get_feature_names())),\n",
    "                                  tfidf_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies one-hot encoding to the provided data, transforming sentences into vectors\n",
    "vectorized_sents = tfidf_vectorizer.transform(adv_brown_sents)\n",
    "\n",
    "# the resulting matrix has shape (N_SAMPLES x VOC_SIZE)\n",
    "vectorized_sents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the sentence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence\n",
    "print(adv_brown_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence vector is almost all zeroes\n",
    "print(vectorized_sents[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonzero elements of the sentence vector\n",
    "print(vectorized_sents[0].nonzero()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the words are the same, but the word order is lost, and stopwords are removed\n",
    "[tfidf_vectorizer_vocab[wid] for wid in vectorized_sents[0].nonzero()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now compute the similarity between sentences like so:\n",
    "sent1vector = vectorized_sents[0]\n",
    "sent10vector = vectorized_sents[10]\n",
    "similarity = cosine_similarity(sent1vector, sent10vector)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot_sentence_similarity(sent1, sent2, vectorizer):\n",
    "    \"\"\"Encodes provided sentences using the 'vectorizer' object,\n",
    "    then computes the cosine similarity between sentence vectors\n",
    "    Outputs a real number between [0,1] \"\"\"\n",
    "    \n",
    "    # CountVectorizer requires a list of sentences as input\n",
    "    sent1 = [sent1]\n",
    "    sent2 = [sent2]\n",
    "    \n",
    "    # your code goes here\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentence_1 = 'I like building robots'\n",
    "test_sentence_2 = 'I also like building pillow fortresses'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert onehot_sentence_similarity(test_sentence_1, test_sentence_2, tfidf_vectorizer) > 0.5\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SearchEngine(object):\n",
    "    def __init__(self, knowledge_base, voc_size=5000):\n",
    "        \"\"\"\n",
    "        Implements a simple information retrieval system based on Tf-Idf text representation.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.kbase = np.array(knowledge_base)\n",
    "        self.vectorizer = TfidfVectorizer(max_features=voc_size)\n",
    "        self.vectorized_kbase = self.vectorizer.fit_transform(knowledge_base)\n",
    "        \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Retrieves the top-k documents from the knowledge_base most similar to given query\n",
    "        \"\"\"\n",
    "        \n",
    "        vectorized_query = self.vectorizer.transform([query])\n",
    "        \n",
    "        # your code goes here\n",
    "        # STEP 1: compute similarities between query and all documents in knowledge base\n",
    "\n",
    "        # STEP 2: sort the similarities to find most similar document indices\n",
    "        # HINT: use np.argsort to do that\n",
    "        \n",
    "        # STEP 3: gets top-k most similar documents from self.kbase, returns them\n",
    "        \n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "se = SearchEngine(adv_brown_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = 'take it easy'\n",
    "result1 = se.search(query1, top_k=1)[0]\n",
    "assert query1 in result1\n",
    "\n",
    "query2 = 'uneasy feeling'\n",
    "result2 = se.search(query2, top_k=1)[0]\n",
    "assert query2 in result2\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you build a 4-gram language model, then use it to generate grammaticaly valid text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))\n",
    " \n",
    "for sentence in tqdm(nltk.corpus.brown.sents()):\n",
    "    for w1, w2, w3 in nltk.trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        # iterate over all trigrams, accumulate co-occurance counts\n",
    "        model[(w1, w2)][w3] += 1\n",
    "\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        # normalize counts to produce a valid probability distribution\n",
    "        model[w1_w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [None, None]\n",
    " \n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "    # introduce a stochastic variable\n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    " \n",
    "    for word in model[tuple(text[-2:])].keys():\n",
    "        pr = model[tuple(text[-2:])][word]\n",
    "        accumulator += pr\n",
    "        \n",
    "        # frequent trigrams are more likely to overflow accumulator:\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    " \n",
    "    if text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    "\n",
    "print(' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 By analogy, implement a 4-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
