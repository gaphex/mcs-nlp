{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bjs = json.load(open(\"./faqs/alfa_faq.02.json\"))\n",
    "\n",
    "traindata = []\n",
    "testdata = []\n",
    "valdata = []\n",
    "\n",
    "\n",
    "for b in bjs:\n",
    "    trainq, testq = train_test_split(b['paraphrased_questions'], test_size=0.5, random_state=34)\n",
    "    valq, testq = train_test_split(testq, test_size=0.5, random_state=34)\n",
    "\n",
    "    b.pop('paraphrase_author')\n",
    "    b1 = deepcopy(b)\n",
    "    b2 = deepcopy(b)\n",
    "    b3 = deepcopy(b)\n",
    "    \n",
    "    b1['paraphrased_questions'] = trainq\n",
    "    traindata.append(b1)\n",
    "    b2['paraphrased_questions'] = testq\n",
    "    testdata.append(b2)\n",
    "    b3['paraphrased_questions'] = valq\n",
    "    valdata.append(b3)\n",
    "    \n",
    "json.dump(traindata, open(\"faq_train.json\",\"w\"))\n",
    "json.dump(testdata, open(\"faq_test.json\",\"w\"))\n",
    "json.dump(valdata, open(\"faq_val.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-based systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-based bots answer user queries by retrieving the most relevant answer from a pre-defined knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import scipy\n",
    "import pymystem3\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knowledge_base = json.load(open(\"../faq_train.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's have a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knowledge_base[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to evaluate your IR engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_score_for_engine(engine, val_json_path):\n",
    "    data = []\n",
    "    for q in json.load(open(val_json_path)):\n",
    "        queries = q['paraphrased_questions']\n",
    "        answer = q['answer']\n",
    "        results = [engine.get_top(query, top_k=3) for query in queries]\n",
    "        for r in results:\n",
    "            data.append([answer] + r)\n",
    "            \n",
    "    ra1 = calc_recall(data, 1)\n",
    "    ra3 = calc_recall(data, 3)\n",
    "    print(\"recall @1: {}\\nrecall @3: {}\".format(ra1, ra3))\n",
    "    return ra1, ra3\n",
    "            \n",
    "def calc_recall(data, k, bootstrap=0, subsample_rate=None):\n",
    "    \"\"\"\n",
    "    :param data: 2d matrix\n",
    "    data[i, 0] - true answer, data[i, 1:] - predicted answers, sorted by decreasing score.\n",
    "    \"\"\"\n",
    "    count = np.zeros(1 + bootstrap)\n",
    "    count_hit = np.zeros(1 + bootstrap)\n",
    "    for fields in data:\n",
    "        query = fields[0]\n",
    "\n",
    "        if subsample_rate is None:\n",
    "            increment = np.random.poisson(lam=1, size=bootstrap)\n",
    "        else:\n",
    "            increment = np.random.binomial(1, subsample_rate, bootstrap)\n",
    "        increment = np.hstack([[1], increment])\n",
    "\n",
    "        if query in fields[1:k+1]:\n",
    "            count_hit += increment\n",
    "        count += increment\n",
    "\n",
    "    recall = count_hit / count\n",
    "\n",
    "    return recall[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we build a basic IR system using a TF-IDF representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ENGINE_1(object):\n",
    "    def __init__(self, kbase_path):\n",
    "        self.knowledge_base = json.load(open(kbase_path))\n",
    "        self.lemmatizer = pymorphy2.MorphAnalyzer()\n",
    "        \n",
    "        # contains correct output for each class\n",
    "        self.answers = np.array([t['answer'] for t in self.knowledge_base])\n",
    "        \n",
    "        self.vectorizer = self.prepare_vectorizer()\n",
    "        self.vectorized_kbase, self.class_indexes = self.vectorize_knowledge_base()\n",
    "        \n",
    "    \n",
    "    def prepare_vectorizer(self):\n",
    "        \"\"\"\n",
    "        Fits TF-IDF vectorizer using all available text from self.knowledge_base\n",
    "        Returns TF-IDF vectorizer object\n",
    "        \"\"\"\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1,2), tokenizer=self.tokenize_and_lemmatize)\n",
    "        all_texts = []\n",
    "        for kb in self.knowledge_base:\n",
    "            all_texts.append(kb['answer'])\n",
    "            all_texts.append(kb['question'])\n",
    "            all_texts += kb['paraphrased_questions']\n",
    "        vectorizer.fit(all_texts)\n",
    "        return vectorizer\n",
    "    \n",
    "    def vectorize(self, data):\n",
    "        \"\"\"\n",
    "        Turns a list of N strings into their vector representation using self.vectorizer.\n",
    "        Returns a a matrix of shape [N, n_features]\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.vectorizer.transform(data)\n",
    "        \n",
    "    def vectorize_knowledge_base(self):\n",
    "        \"\"\"\n",
    "        Vectorizes all questions using the vectorize function.\n",
    "        Builds a list containing class number for each question.        \n",
    "        \"\"\"\n",
    "        questions = [t['question'] for t in self.knowledge_base]\n",
    "        return self.vectorize(questions), list(range(len(self.knowledge_base)))\n",
    "    \n",
    "    def compute_class_scores(self, similarities):\n",
    "        \"\"\"\n",
    "        Accepts an array of similarities of shape (self.class_indexes, )\n",
    "        Computes scores for classes.\n",
    "        Returns a dictionary of size (n_classes) that looks like\n",
    "        {\n",
    "            0: 0.3,\n",
    "            1: 0.1,\n",
    "            2: 0.0,\n",
    "            class_n_id: class_n_score\n",
    "            ...\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        class_scores = dict(zip(range(len(self.answers)), similarities))\n",
    "        return class_scores\n",
    "        \n",
    "    def tokenize_and_lemmatize(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        for tk in tokens:\n",
    "\n",
    "                try:\n",
    "                    lemma = self.lemmatizer.parse(tk)[0].normal_form\n",
    "                    tokens.append(lemma)\n",
    "                except IndexError:\n",
    "                    tokens.append(tk)\n",
    "        return tokens\n",
    "    \n",
    "    def get_top(self, query, top_k=3):\n",
    "        if isinstance(query, str):\n",
    "            query = [query]\n",
    "            \n",
    "        vectorized_query = self.vectorize(query)\n",
    "        css = cosine_similarity(vectorized_query, self.vectorized_kbase)[0]\n",
    "        scores = self.compute_class_scores(css)\n",
    "        \n",
    "        sorted_scores = sorted(scores.items(), key= lambda x: x[1])[::-1][:top_k]\n",
    "        top_classes = np.array([c[0] for c in sorted_scores])\n",
    "        top_answers = list(self.answers[top_classes])\n",
    "        return top_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine1 = ENGINE_1(\"../faq_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare vectorizer OK\n"
     ]
    }
   ],
   "source": [
    "t1 = engine1.prepare_vectorizer()\n",
    "assert isinstance(t1, TfidfVectorizer)\n",
    "assert len(t1.get_feature_names()) > 1500\n",
    "print(\"prepare vectorizer OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorize OK\n"
     ]
    }
   ],
   "source": [
    "t2 = engine1.vectorize(['Как получить ЗП-карту?',\n",
    "   'А можно зарплатную карту вне очереди получить - очень надо?',\n",
    "   'Как быть с зарплатой?'])\n",
    "\n",
    "assert isinstance(t2, scipy.sparse.csr_matrix)\n",
    "assert t2.shape[0] == 3 and t2.shape[1] == len(engine1.vectorizer.get_feature_names())\n",
    "print(\"vectorize OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_class_scores OK\n"
     ]
    }
   ],
   "source": [
    "t3 = engine1.compute_class_scores(np.arange(0,1,0.034))\n",
    "assert isinstance(t3, dict)\n",
    "assert list(t3.items()) == list(zip(range(30), np.arange(0,1,0.034)))\n",
    "print(\"compute_class_scores OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall @1: 0.578125\n",
      "recall @3: 0.8072916666666666\n",
      "scores OK\n"
     ]
    }
   ],
   "source": [
    "r1, r3 = calc_score_for_engine(engine1, \"./faq_val.json\")\n",
    "assert r1 >0.5\n",
    "assert r3 >0.75\n",
    "print(\"scores OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we improve our IR system using word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(\"../word_vectors.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_encoder(wmodel, tokenizer, text, vsize=300):\n",
    "    \"\"\"\n",
    "    This function encodes text into a vector.\n",
    "    \n",
    "    First, it tokenizes input text using the provided tokenizer function.\n",
    "    Then it uses the provided word2vec model to get the vectors corresponding to text's tokens.\n",
    "    Finally, it computes an average of all token's vectors and returns it.\n",
    "    \n",
    "    If the function failed to find and encode any words, it should at least return a vector of zeros.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text)\n",
    "    \n",
    "    zero_vector = np.zeros(vsize)\n",
    "    word_vectors = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in wmodel:\n",
    "            word_vectors.append(wmodel[token])\n",
    "            \n",
    "    if len(word_vectors):\n",
    "        sent_vector = np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        sent_vector = zero_vector\n",
    "    # your code goes here\n",
    "    return sent_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ENGINE_2(object):\n",
    "    def __init__(self, kbase_path, w2v_model):\n",
    "        self.knowledge_base = json.load(open(kbase_path))\n",
    "        self.lemmatizer = pymystem3.Mystem()\n",
    "        self.w2v_model = w2v_model\n",
    "        \n",
    "        # contains correct output for each class\n",
    "        self.answers = np.array([t['answer'] for t in self.knowledge_base])\n",
    "        \n",
    "        self.vectorized_kbase, self.class_indexes = self.vectorize_knowledge_base()\n",
    "    \n",
    "    def vectorize(self, data):\n",
    "        \"\"\"\n",
    "        Turns a list of N strings into their vector representation using self.w2v_model.\n",
    "        In the simplest case, averages the word vectors of all words in a sentence.\n",
    "        Returns a a matrix of shape [N, 300] (300 = word vector dimensionality)\n",
    "        \"\"\"\n",
    "        vectorized = []\n",
    "        for d in data:\n",
    "            vectorized.append(bow_encoder(self.w2v_model, self.tokenize_and_lemmatize, d))\n",
    "        \n",
    "        return np.array(vectorized)\n",
    "        \n",
    "    def vectorize_knowledge_base(self):\n",
    "        \"\"\"\n",
    "        Vectorizes all questions using the vectorize function.\n",
    "        Builds a list containing class number for each question.        \n",
    "        \"\"\"\n",
    "        questions = [t['question'] for t in self.knowledge_base]\n",
    "        return self.vectorize(questions), list(range(len(self.knowledge_base)))\n",
    "    \n",
    "    def compute_class_scores(self, similarities):\n",
    "        \"\"\"\n",
    "        Accepts an array of similarities of shape (self.class_indexes, )\n",
    "        Computes scores for classes.\n",
    "        Returns a dictionary of size (n_classes) that looks like\n",
    "        {\n",
    "            0: 0.3,\n",
    "            1: 0.1,\n",
    "            2: 0.0,\n",
    "            class_n_id: class_n_score\n",
    "            ...\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        class_scores = dict(zip(range(len(self.answers)), similarities))\n",
    "        return class_scores\n",
    "        \n",
    "    def tokenize_and_lemmatize(self, text):\n",
    "        analysis = self.lemmatizer.analyze(text.strip())\n",
    "        tokens = []\n",
    "        for an in analysis:\n",
    "            if 'analysis' in an:\n",
    "                try:\n",
    "                    tokens.append(an['analysis'][0]['lex'])\n",
    "                except IndexError:\n",
    "                    tokens.append(an['text'])\n",
    "        return tokens\n",
    "    \n",
    "    def get_top(self, query, top_k=3):\n",
    "        if isinstance(query, str):\n",
    "            query = [query]\n",
    "            \n",
    "        vectorized_query = self.vectorize(query)\n",
    "        css = cosine_similarity(vectorized_query, self.vectorized_kbase)[0]\n",
    "        scores = self.compute_class_scores(css)\n",
    "        \n",
    "        sorted_scores = sorted(scores.items(), key= lambda x: x[1])[::-1][:top_k]\n",
    "        top_classes = np.array([c[0] for c in sorted_scores])\n",
    "        top_answers = list(self.answers[top_classes])\n",
    "        return top_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine2 = ENGINE_2(\"./faq_train.json\", w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorize OK\n"
     ]
    }
   ],
   "source": [
    "t1 = engine2.vectorize(['Как получить ЗП-карту?',\n",
    "   'А можно зарплатную карту вне очереди получить - очень надо?',\n",
    "   'Как быть с зарплатой?'])\n",
    "\n",
    "assert isinstance(t1, np.ndarray)\n",
    "assert t1.shape[0] == 3 and t1.shape[1] == 300\n",
    "print(\"vectorize OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall @1: 0.6666666666666666\n",
      "recall @3: 0.8541666666666666\n",
      "scores OK\n"
     ]
    }
   ],
   "source": [
    "r1, r3 = calc_score_for_engine(engine2, \"./faq_val.json\")\n",
    "assert r1 >0.65\n",
    "assert r3 >0.8\n",
    "print(\"scores OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we use available paraphrases to further improve the quiality of our IR system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ENGINE_3(object):\n",
    "    def __init__(self, kbase_path, w2v_model):\n",
    "        self.knowledge_base = json.load(open(kbase_path))\n",
    "        self.lemmatizer = pymystem3.Mystem()\n",
    "        self.w2v_model = w2v_model\n",
    "        \n",
    "        # contains correct output for each class\n",
    "        self.answers = np.array([t['answer'] for t in self.knowledge_base])\n",
    "        \n",
    "        self.vectorized_kbase, self.class_indexes = self.vectorize_knowledge_base()\n",
    "    \n",
    "    def vectorize(self, data):\n",
    "        \"\"\"\n",
    "        Turns a list of N strings into their vector representation using self.w2v_model.\n",
    "        In the simplest case, averages the word vectors of all words in a sentence.\n",
    "        Returns a a matrix of shape [N, 300]\n",
    "        \"\"\"\n",
    "        vectorized = []\n",
    "        for d in data:\n",
    "            vectorized.append(bow_encoder(self.w2v_model, self.tokenize_and_lemmatize, d))\n",
    "        \n",
    "        return np.array(vectorized)\n",
    "        \n",
    "    def vectorize_knowledge_base(self):\n",
    "        \"\"\"\n",
    "        Vectorizes all questions using the vectorize function.\n",
    "        Builds a list containing class number for each question.        \n",
    "        \"\"\"\n",
    "        vectors = []\n",
    "        class_labels = []\n",
    "        \n",
    "        for i, t in enumerate(self.knowledge_base):\n",
    "            vc = np.vstack([self.vectorize([t['question']]),\n",
    "                            self.vectorize(t['paraphrased_questions'])])\n",
    "            \n",
    "            vectors.append(vc)\n",
    "            class_labels.append(i)\n",
    "            class_labels += [i]*len(t['paraphrased_questions'])\n",
    "        \n",
    "        \n",
    "        return np.vstack(vectors), class_labels\n",
    "    \n",
    "    def compute_class_scores(self, similarities):\n",
    "        \"\"\"\n",
    "        Accepts an array of similarities of shape (self.class_indexes, )\n",
    "        Computes scores for classes.\n",
    "        Returns a dictionary of size (n_classes) that looks like\n",
    "        {\n",
    "            0: 0.3,\n",
    "            1: 0.1,\n",
    "            2: 0.0,\n",
    "            class_n_id: class_n_score\n",
    "            ...\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        class_scores = dict(zip(range(len(self.answers)), [0]*len(self.answers)))\n",
    "        \n",
    "        for ci, sc in zip(self.class_indexes, similarities):\n",
    "            class_scores[ci] += sc\n",
    "        return class_scores\n",
    "        \n",
    "    def tokenize_and_lemmatize(self, text):\n",
    "        analysis = self.lemmatizer.analyze(text.strip())\n",
    "        tokens = []\n",
    "        for an in analysis:\n",
    "            if 'analysis' in an:\n",
    "                try:\n",
    "                    tokens.append(an['analysis'][0]['lex'])\n",
    "                except IndexError:\n",
    "                    tokens.append(an['text'])\n",
    "        return tokens\n",
    "    \n",
    "    def get_top(self, query, top_k=3):\n",
    "        if isinstance(query, str):\n",
    "            query = [query]\n",
    "            \n",
    "        vectorized_query = self.vectorize(query)\n",
    "        css = cosine_similarity(vectorized_query, self.vectorized_kbase)[0]\n",
    "        scores = self.compute_class_scores(css)\n",
    "        \n",
    "        sorted_scores = sorted(scores.items(), key= lambda x: x[1])[::-1][:top_k]\n",
    "        top_classes = np.array([c[0] for c in sorted_scores])\n",
    "        top_answers = list(self.answers[top_classes])\n",
    "        return top_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine3 = ENGINE_3(\"../faq_train.json\", w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorize knowledge base OK\n"
     ]
    }
   ],
   "source": [
    "t1 = engine3.vectorize_knowledge_base()\n",
    "\n",
    "assert isinstance(t1[0], np.ndarray)\n",
    "assert t1[0].shape[0] == len([t['answer'] for t in engine3.knowledge_base]) +len(list(chain.from_iterable(\n",
    "    [t['paraphrased_questions'] for t in engine3.knowledge_base])))\n",
    "\n",
    "print(\"vectorize knowledge base OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall @1: 0.7395833333333334\n",
      "recall @3: 0.8958333333333334\n",
      "scores OK\n"
     ]
    }
   ],
   "source": [
    "r1, r3 = calc_score_for_engine(engine3, \"../faq_val.json\")\n",
    "assert r1 >0.70\n",
    "assert r3 >0.85\n",
    "print(\"scores OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-02 14:01:23,671 - __main__ - INFO - Im alive!\n",
      "2017-12-02 14:01:23,671 - __main__ - INFO - Im alive!\n",
      "2017-12-02 14:01:23,671 - __main__ - INFO - Im alive!\n",
      "2017-12-02 14:01:23,671 - __main__ - INFO - Im alive!\n",
      "2017-12-02 14:01:23,671 - __main__ - INFO - Im alive!\n",
      "2017-12-02 14:01:29,250 - __main__ - INFO - get_answer recieved message: /answer this\n",
      "2017-12-02 14:01:29,250 - __main__ - INFO - get_answer recieved message: /answer this\n",
      "2017-12-02 14:01:29,250 - __main__ - INFO - get_answer recieved message: /answer this\n",
      "2017-12-02 14:01:29,250 - __main__ - INFO - get_answer recieved message: /answer this\n",
      "2017-12-02 14:01:29,250 - __main__ - INFO - get_answer recieved message: /answer this\n",
      "2017-12-02 14:01:55,840 - __main__ - INFO - get_answer recieved message: /answer где мне получить зарплату\n",
      "2017-12-02 14:01:55,840 - __main__ - INFO - get_answer recieved message: /answer где мне получить зарплату\n",
      "2017-12-02 14:01:55,840 - __main__ - INFO - get_answer recieved message: /answer где мне получить зарплату\n",
      "2017-12-02 14:01:55,840 - __main__ - INFO - get_answer recieved message: /answer где мне получить зарплату\n",
      "2017-12-02 14:01:55,840 - __main__ - INFO - get_answer recieved message: /answer где мне получить зарплату\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters\n",
    "\n",
    "from config import TOKEN, LOG_FILE\n",
    "\n",
    "\n",
    "# Enable logging\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# Logging to file\n",
    "fh = logging.FileHandler(LOG_FILE)\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "# Logging to console\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "\n",
    "class Bot:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.updater = Updater(TOKEN)\n",
    "        self.dsp = self.updater.dispatcher\n",
    "\n",
    "        # register handler functions which define how the bot reacts to events\n",
    "        self.dsp.add_handler(CommandHandler(\"start\", get_help))\n",
    "        self.dsp.add_handler(CommandHandler(\"help\", get_help))\n",
    "        self.dsp.add_handler(CommandHandler(\"sentiment\", get_sentiment))\n",
    "        self.dsp.add_handler(CommandHandler(\"answer\", get_answer))\n",
    "        self.dsp.add_handler(MessageHandler(Filters.text, echo))\n",
    "        self.dsp.add_error_handler(error)\n",
    "\n",
    "        logger.info('Im alive!')\n",
    "\n",
    "    def power_on(self):\n",
    "        # start the Bot\n",
    "        self.updater.start_polling()\n",
    "        self.updater.idle()\n",
    "\n",
    "# define command handlers. These usually take the two arguments: bot and\n",
    "# update. Error handlers also receive the raised TelegramError object in error.\n",
    "\n",
    "\n",
    "def echo(bot, update):\n",
    "    logger.info('echo recieved message: {}'.format(update.message.text))\n",
    "    bot.sendMessage(update.message.chat_id, text=update.message.text)\n",
    "\n",
    "\n",
    "def error(bot, update, error):\n",
    "    # all uncaught telegram-related exceptions will be rerouted here\n",
    "    logger.error('Update \"%s\" caused error \"%s\"' % (update, error))\n",
    "\n",
    "\n",
    "def get_help(bot, update):\n",
    "    logger.info('get_help recieved message: {}'.format(update.message.text))\n",
    "    help_msg = ('Greetings, {} {}! Name is {}, at your service.\\n'\n",
    "                'I currently support the following commands:\\n'\n",
    "                '/start - begins our chat and prints this message\\n'\n",
    "                '/help - prints this message\\n'\n",
    "                '/sentiment [message] - predicts the sentiment of the message').format(\n",
    "        update.message.from_user.first_name, update.message.from_user.last_name, bot.name)\n",
    "    bot.sendMessage(update.message.chat_id, text=help_msg)\n",
    "\n",
    "\n",
    "def get_sentiment(bot, update):\n",
    "    logger.info('get_sentiment recieved message: {}'.format(update.message.text))\n",
    "    try:\n",
    "        # get message text without the command '/sentiment'\n",
    "        usr_msg = update.message.text.split(' ', maxsplit=1)[1]\n",
    "        msg_sentiment = 0.5\n",
    "        '''\n",
    "        Now determine the sentiment of usr_msg.\n",
    "        This should return a real number in [0,1].\n",
    "        Your code goes here.\n",
    "        '''\n",
    "        bot.sendMessage(update.message.chat_id, text=msg_sentiment)\n",
    "    except IndexError:\n",
    "        bot.sendMessage(update.message.chat_id, text='Write your message after the command')\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        \n",
    "def get_answer(bot, update):\n",
    "    logger.info('get_answer recieved message: {}'.format(update.message.text))\n",
    "    try:\n",
    "        # get message text without the command \n",
    "        usr_msg = update.message.text.split(' ', maxsplit=1)[1]\n",
    "        answer = engine3.get_top(usr_msg)[0]\n",
    "        '''\n",
    "        Now determine the sentiment of usr_msg.\n",
    "        This should return a real number in [0,1].\n",
    "        Your code goes here.\n",
    "        '''\n",
    "        bot.sendMessage(update.message.chat_id, text=answer)\n",
    "    except IndexError:\n",
    "        bot.sendMessage(update.message.chat_id, text='Write your message after the command')\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "\n",
    "my_bot = Bot()\n",
    "my_bot.power_on()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
