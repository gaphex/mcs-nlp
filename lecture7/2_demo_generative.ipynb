{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You need tensorflow == 0.12 for this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert tf.__version__ == '0.12.0'\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import seq2seq\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from generative_utils import (get_paths, initial_state_with_relevance_masking,\n",
    "                              forward_text, forward_with_mask, beam_search_generator)\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, args, infer=False): # infer is set to true during sampling.\n",
    "        self.args = args\n",
    "        if infer:\n",
    "            # Worry about one character at a time during sampling; no batching or BPTT.\n",
    "            args.batch_size = 1\n",
    "            args.seq_length = 1\n",
    "\n",
    "        # Set cell_fn to the type of network cell we're creating -- RNN, GRU or LSTM.\n",
    "        if args.model == 'rnn':\n",
    "            cell_fn = rnn_cell.BasicRNNCell\n",
    "        elif args.model == 'gru':\n",
    "            cell_fn = rnn_cell.GRUCell\n",
    "        elif args.model == 'lstm':\n",
    "            cell_fn = rnn_cell.BasicLSTMCell\n",
    "        else:\n",
    "            raise Exception(\"model type not supported: {}\".format(args.model))\n",
    "\n",
    "        # Call tensorflow library tensorflow-master/tensorflow/python/ops/rnn_cell\n",
    "        # to create a layer of rnn_size cells of the specified basic type (RNN/GRU/LSTM).\n",
    "        cell = cell_fn(args.rnn_size)\n",
    "\n",
    "        # Use the same rnn_cell library to create a stack of these cells\n",
    "        # of num_layers layers. Pass in a python list of these cells.\n",
    "        # (The [cell] * arg.num_layers syntax literally duplicates cell multiple times in\n",
    "        # a list. The syntax is such that [5, 6] * 3 would return [5, 6, 5, 6, 5, 6].)\n",
    "        self.cell = cell = rnn_cell.MultiRNNCell([cell] * args.num_layers)\n",
    "\n",
    "        # Create two TF placeholder nodes of 32-bit ints (NOT floats!),\n",
    "        # each of shape batch_size x seq_length. This shape matches the batches\n",
    "        # (listed in x_batches and y_batches) constructed in create_batches in utils.py.\n",
    "        # input_data will receive input batches, and targets will be what it compares against\n",
    "        # to calculate loss.\n",
    "        self.input_data = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\n",
    "        self.targets = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\n",
    "\n",
    "        # Using the zero_state function in the RNNCell master class in rnn_cell library,\n",
    "        # create a tensor of zeros such that we can swap it in for the network state at any time\n",
    "        # to zero out the network's state.\n",
    "        # State dimensions are: cell_fn state size (2 for LSTM) x rnn_size x num_layers.\n",
    "        # So an LSTM network with 100 cells per layer and 3 layers would have a state size of 600,\n",
    "        # and initial_state would have a dimension of none x 600.\n",
    "        self.initial_state = self.cell.zero_state(args.batch_size, tf.float32)\n",
    "\n",
    "        # Scope our new variables to the scope identifier string \"rnnlm\".\n",
    "        with tf.variable_scope('rnnlm'):\n",
    "            # Create new variable softmax_w and softmax_b for output.\n",
    "            # softmax_w is a weights matrix from the top layer of the model (of size rnn_size)\n",
    "            # to the vocabulary output (of size vocab_size).\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [args.rnn_size, args.vocab_size])\n",
    "            # softmax_b is a bias vector of the ouput characters (of size vocab_size).\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [args.vocab_size])\n",
    "\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                # Create new variable named 'embedding' to connect the character input to the base layer\n",
    "                # of the RNN. Its role is the conceptual inverse of softmax_w.\n",
    "                # It contains the trainable weights from the one-hot input vector to the lowest layer of RNN.\n",
    "                embedding = tf.get_variable(\"embedding\", [args.vocab_size, args.rnn_size])\n",
    "                # Create an embedding tensor with tf.nn.embedding_lookup(embedding, self.input_data).\n",
    "                # This tensor has dimensions batch_size x seq_length x rnn_size.\n",
    "                # tf.split splits that embedding lookup tensor into seq_length tensors (along dimension 1).\n",
    "                # Thus inputs is a list of seq_length different tensors,\n",
    "                # each of dimension batch_size x 1 x rnn_size.\n",
    "                inputs = tf.split(1, args.seq_length, tf.nn.embedding_lookup(embedding, self.input_data))\n",
    "                # Iterate through these resulting tensors and eliminate that degenerate second dimension of 1,\n",
    "                # i.e. squeeze each from batch_size x 1 x rnn_size down to batch_size x rnn_size.\n",
    "                # Thus we now have a list of seq_length tensors, each with dimension batch_size x rnn_size.\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "\n",
    "        # This looping function is used as part of seq2seq.rnn_decoder only during sampling -- not training.\n",
    "        # prev is a 2D Tensor of shape [batch_size x cell.output_size].\n",
    "        # returns a 2D Tensor of shape [batch_size x cell.input_size].\n",
    "        def loop(prev, _):\n",
    "            # prev is initially the top cell state.\n",
    "            # Convert the top cell state into character logits.\n",
    "            prev = tf.matmul(prev, softmax_w) + softmax_b\n",
    "            # Pull the character with the greatest logit (no sampling, just argmaxing).\n",
    "            # WHY IS THIS ARGMAXING WHEN ACTUAL SAMPLING IS DONE PROBABILISTICALLY?\n",
    "            # DOESN'T THIS CAUSE OUTPUTS NOT TO MATCH INPUTS DURING SEQUENCE GENERATION?\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "            # Re-embed that symbol as the next step's input, and return that.\n",
    "            return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "\n",
    "        # Set up a seq2seq decoder from the seq2seq.py library.\n",
    "        # This constructs the outputs and states nodes of the network.\n",
    "        # Outputs is a list (of len seq_length, same as inputs) of tensors of shape [batch_size x rnn_size].\n",
    "        # These are the raw output values of the top layer of the network at each time step.\n",
    "        # They have NOT been fed through the decoder projection; they are still in network space,\n",
    "        # not character space.\n",
    "        # State is a tensor of shape [batch_size x cell.state_size].\n",
    "        # This is also the step where all of the trainable parameters for the LSTM (weights and biases) are defined.\n",
    "        outputs, self.final_state = seq2seq.rnn_decoder(inputs,\n",
    "                self.initial_state, cell, loop_function=loop if infer else None, scope='rnnlm')\n",
    "        # tf.concat concatenates the output tensors along the rnn_size dimension,\n",
    "        # to make a single tensor of shape [batch_size x (seq_length * rnn_size)].\n",
    "        # This gives the following 2D outputs matrix:\n",
    "        #   [(rnn output: batch 0, seq 0) (rnn output: batch 0, seq 1) ... (rnn output: batch 0, seq seq_len-1)]\n",
    "        #   [(rnn output: batch 1, seq 0) (rnn output: batch 1, seq 1) ... (rnn output: batch 1, seq seq_len-1)]\n",
    "        #   ...\n",
    "        #   [(rnn output: batch batch_size-1, seq 0) (rnn output: batch batch_size-1, seq 1) ... (rnn output: batch batch_size-1, seq seq_len-1)]\n",
    "        # tf.reshape then reshapes it to a tensor of shape [(batch_size * seq_length) x rnn_size].\n",
    "        # Output will now be the following matrix:\n",
    "        #   [rnn output: batch 0, seq 0]\n",
    "        #   [rnn output: batch 0, seq 1]\n",
    "        #   ...\n",
    "        #   [rnn output: batch 0, seq seq_len-1]\n",
    "        #   [rnn output: batch 1, seq 0]\n",
    "        #   [rnn output: batch 1, seq 1]\n",
    "        #   ...\n",
    "        #   [rnn output: batch 1, seq seq_len-1]\n",
    "        #   ...\n",
    "        #   ...\n",
    "        #   [rnn output: batch batch_size-1, seq seq_len-1]\n",
    "        # Note the following comment in rnn_cell.py:\n",
    "        #   Note: in many cases it may be more efficient to not use this wrapper,\n",
    "        #   but instead concatenate the whole sequence of your outputs in time,\n",
    "        #   do the projection on this batch-concatenated sequence, then split it\n",
    "        #   if needed or directly feed into a softmax.\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1, args.rnn_size])\n",
    "        # Obtain logits node by applying output weights and biases to the output tensor.\n",
    "        # Logits is a tensor of shape [(batch_size * seq_length) x vocab_size].\n",
    "        # Recall that outputs is a 2D tensor of shape [(batch_size * seq_length) x rnn_size],\n",
    "        # and softmax_w is a 2D tensor of shape [rnn_size x vocab_size].\n",
    "        # The matrix product is therefore a new 2D tensor of [(batch_size * seq_length) x vocab_size].\n",
    "        # In other words, that multiplication converts a loooong list of rnn_size vectors\n",
    "        # to a loooong list of vocab_size vectors.\n",
    "        # Then add softmax_b (a single vocab-sized vector) to every row of that list.\n",
    "        # That gives you the logits!\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        # Convert logits to probabilities. Probs isn't used during training! That node is never calculated.\n",
    "        # Like logits, probs is a tensor of shape [(batch_size * seq_length) x vocab_size].\n",
    "        # During sampling, this means it is of shape [1 x vocab_size].\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        # seq2seq.sequence_loss_by_example returns 1D float Tensor containing the log-perplexity\n",
    "        # for each sequence. (Size is batch_size * seq_length.)\n",
    "        # Targets are reshaped from a [batch_size x seq_length] tensor to a 1D tensor, of the following layout:\n",
    "        #   target character (batch 0, seq 0)\n",
    "        #   target character (batch 0, seq 1)\n",
    "        #   ...\n",
    "        #   target character (batch 0, seq seq_len-1)\n",
    "        #   target character (batch 1, seq 0)\n",
    "        #   ...\n",
    "        # These targets are compared to the logits to generate loss.\n",
    "        # Logits: instead of a list of character indices, it's a list of character index probability vectors.\n",
    "        # seq2seq.sequence_loss_by_example will do the work of generating losses by comparing the one-hot vectors\n",
    "        # implicitly represented by the target characters against the probability distrutions in logits.\n",
    "        # It returns a 1D float tensor (a vector) where item i is the log-perplexity of\n",
    "        # the comparison of the ith logit distribution to the ith one-hot target vector.\n",
    "        loss = seq2seq.sequence_loss_by_example([self.logits], # logits: 1-item list of 2D Tensors of shape [batch_size x vocab_size]\n",
    "                [tf.reshape(self.targets, [-1])], # targets: 1-item list of 1D batch-sized int32 Tensors of the same length as logits\n",
    "                [tf.ones([args.batch_size * args.seq_length])], # weights: 1-item list of 1D batch-sized float-Tensors of the same length as logits\n",
    "                args.vocab_size) # num_decoder_symbols: integer, number of decoder symbols (output classes)\n",
    "        # Cost is the arithmetic mean of the values of the loss tensor\n",
    "        # (the sum divided by the total number of elements).\n",
    "        # It is a single-element floating point tensor. This is what the optimizer seeks to minimize.\n",
    "        self.cost = tf.reduce_sum(loss) / args.batch_size / args.seq_length\n",
    "        # Create a summary for our cost.\n",
    "        tf.scalar_summary(\"cost\", self.cost)\n",
    "        # Create a node to track the learning rate as it decays through the epochs.\n",
    "        self.lr = tf.Variable(args.learning_rate, trainable=False)\n",
    "        self.global_epoch_fraction = tf.Variable(0.0, trainable=False)\n",
    "        self.global_seconds_elapsed = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables() # tvars is a python list of all trainable TF Variable objects.\n",
    "\n",
    "        # tf.gradients returns a list of tensors of length len(tvars) where each tensor is sum(dy/dx).\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n",
    "                args.grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr) # Use ADAM optimizer with the current learning rate.\n",
    "        # Zip creates a list of tuples, where each tuple is (variable tensor, gradient tensor).\n",
    "        # Training op nudges the variables along the gradient, with the given learning rate, using the ADAM optimizer.\n",
    "        # This is the op that a training session should be instructed to perform.\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        self.summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    def save_variables_list(self):\n",
    "        # Return a list of the trainable variables created within the rnnlm model.\n",
    "        # This consists of the two projection softmax variables (softmax_w and softmax_b),\n",
    "        # embedding, and all of the weights and biases in the MultiRNNCell model.\n",
    "        # Save only the trainable variables and the placeholders needed to resume training;\n",
    "        # discard the rest, including optimizer state.\n",
    "        save_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='rnnlm')\n",
    "        save_vars.append(self.lr)\n",
    "        save_vars.append(self.global_epoch_fraction)\n",
    "        save_vars.append(self.global_seconds_elapsed)\n",
    "        return save_vars\n",
    "\n",
    "    def forward_model(self, sess, state, input_sample):\n",
    "        '''Run a forward pass. Return the updated hidden state and the output probabilities.'''\n",
    "        shaped_input = np.array([[input_sample]], np.float32)\n",
    "        inputs = {self.input_data: shaped_input,\n",
    "                    self.initial_state: state}\n",
    "        [probs, state] = sess.run([self.probs, self.final_state], feed_dict=inputs)\n",
    "        return probs[0], state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. download pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/5f5rq36r62cvf0a/reddit.zip?dl=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. unzip model to directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path, config_path, vocab_path = get_paths('./reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_information_retrieval.ipynb  faq_train.json\t    reddit\r\n",
      "complete\t\t       faq_val.json\t    Untitled.ipynb\r\n",
      "demo_generative.ipynb\t       generative\t    word_vectors.w2v\r\n",
      "dials.pkl\t\t       generative_utils.py\r\n",
      "faq_test.json\t\t       __pycache__\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generative_Model(object):\n",
    "    def __init__(self, asset_path):\n",
    "        model_path, config_path, vocab_path = get_paths(asset_path)\n",
    "        \n",
    "\n",
    "        saved_args = Namespace(**json.load(open(os.path.join(asset_path, \"args.json\"))))\n",
    "        # Separately load chars and vocab from the save directory.\n",
    "            \n",
    "\n",
    "        self.chars, self.vocab = json.load(open(os.path.join(asset_path, \"voc.json\")))\n",
    "        # Create the model from the saved arguments, in inference mode.\n",
    "            \n",
    "        print(\"Creating model...\")\n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            self.net = Model(saved_args, True)\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allow_growth = True\n",
    "\n",
    "            self.sess = tf.Session(config=config, graph=self.graph)\n",
    "\n",
    "            tf.initialize_all_variables().run(session=self.sess)\n",
    "            saver = tf.train.Saver(self.net.save_variables_list())\n",
    "                        # Restore the saved variables, replacing the initialized values.\n",
    "            print(\"Restoring weights...\")\n",
    "            saver.restore(self.sess, model_path)\n",
    "\n",
    "            self.max_length = 500\n",
    "            self.beam_width = 2\n",
    "            self.relevance = 0.3\n",
    "            self.temperature = 1.0\n",
    "            self.states = initial_state_with_relevance_masking(self.net, self.sess, self.relevance)\n",
    "        \n",
    "    def answer_to(self, user_input):\n",
    "\n",
    "            buf = []\n",
    "            self.states = forward_text(self.net, self.sess, self.states, self.vocab, '> ' + user_input + \"\\n>\")\n",
    "            computer_response_generator = beam_search_generator(sess=self.sess, net=self.net,\n",
    "                initial_state=copy.deepcopy(self.states), initial_sample=self.vocab[' '],\n",
    "                early_term_token=self.vocab['\\n'], beam_width=self.beam_width, forward_model_fn=forward_with_mask,\n",
    "                forward_args=(self.relevance, self.vocab['\\n']), temperature=self.temperature)\n",
    "            for i, char_token in enumerate(computer_response_generator):\n",
    "                buf.append(self.chars[char_token])\n",
    "                self.states = forward_text(self.net, self.sess, self.states, self.vocab, self.chars[char_token])\n",
    "                if i >= self.max_length: break\n",
    "            self.states = forward_text(self.net, self.sess, self.states, self.vocab, '\\n> ')\n",
    "            return ''.join(buf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "WARNING:tensorflow:From <ipython-input-7-67b1767d4a7e>:158 in __init__.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-7-67b1767d4a7e>:173 in __init__.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "WARNING:tensorflow:From /home/aphex/.local/lib/python3.5/site-packages/tensorflow/python/ops/logging_ops.py:264 in merge_all_summaries.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "WARNING:tensorflow:From <ipython-input-14-a2154187903e>:24 in __init__.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Restoring weights...\n"
     ]
    }
   ],
   "source": [
    "hm = Generative_Model(\"./reddit/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I didn't expect anything.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm.answer_to(\"What are you up to?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
